{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73ddb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "import copy\n",
    "from torch.distributions import Categorical, kl_divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3715a6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: Tesla P100-PCIE-16GB\n",
      "Total GPU Memory: 15.89 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    \n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    total_memory_gb = gpu_properties.total_memory / (1024 ** 3)\n",
    "    \n",
    "    print(\"GPU detected:\", gpu_name)\n",
    "    print(f\"Total GPU Memory: {total_memory_gb:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463ea9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, Cache\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Callable\n",
    "# import flash_attn\n",
    "\n",
    "def _sample_top_p(logits, top_p=0.9):\n",
    "   \n",
    "    logits = logits - torch.max(logits)\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    eps = 1e-10\n",
    "    probs = torch.clamp(probs, min=eps)\n",
    "    probs = probs / probs.sum()  \n",
    "    \n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    \n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    # Remove tokens with cumulative probability above the threshold\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    # Keep the first token above threshold\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    \n",
    "    # Scatter back to original indices\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "    probs = probs.masked_fill(indices_to_remove, 0.0)\n",
    "    \n",
    "    # Re-normalize after masking\n",
    "    probs = probs / (probs.sum() + eps)\n",
    "    \n",
    "    # Check for invalid values before sampling\n",
    "    if torch.isnan(probs).any() or torch.isinf(probs).any() or (probs < 0).any():\n",
    "        # Fix invalid values\n",
    "        probs = torch.nan_to_num(probs, nan=eps, posinf=1.0, neginf=eps)\n",
    "        probs = torch.clamp(probs, min=eps)\n",
    "        probs = probs / probs.sum()  # Re-normalize\n",
    "    \n",
    "    # Sample from the filtered distribution\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    return next_token\n",
    "\n",
    "class WIMInference:\n",
    "\n",
    "    def __init__(self, model, tokenizer) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.wim_kv_cache = DynamicCache()\n",
    "        self.classifier_kv_cache = DynamicCache()\n",
    "\n",
    "    def _prefill_tokens(self,input_ids: torch.Tensor,attention_mask: torch.Tensor,cache_positions: torch.Tensor,kv_cache: Cache,):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                cache_position=cache_positions,\n",
    "                use_cache=True,\n",
    "                past_key_values=kv_cache,\n",
    "            )\n",
    "        # print(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def shrink_kv_cache_from_end(self, new_size: int, kv_cache: Cache):\n",
    "\n",
    "        def resize_tensor_list(token_list):\n",
    "            for layer_idx in range(len(token_list)):\n",
    "                token_list[layer_idx] = token_list[layer_idx][:, :, :new_size, :]\n",
    "\n",
    "        resize_tensor_list(kv_cache.key_cache)\n",
    "        resize_tensor_list(kv_cache.value_cache)\n",
    "        kv_cache._seen_tokens = new_size\n",
    "\n",
    "    def generate_text_with_kv_cache(self,max_new_tokens: int,previous_logits: torch.Tensor,do_sample: bool,top_p: float,temperature: float,early_stopping: bool,kv_cache: Cache,) -> str:\n",
    "        generated_tokens = []\n",
    "\n",
    "        next_token_pos = kv_cache.get_seq_length()\n",
    "\n",
    "        logits = previous_logits\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Select the last token from the logits\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            if do_sample:\n",
    "                \n",
    "                next_token_logits = next_token_logits / temperature\n",
    "                \n",
    "                next_token_probs = torch.nn.functional.softmax(\n",
    "                    next_token_logits, dim=-1\n",
    "                )\n",
    "                \n",
    "                # Check for invalid values (moved here after next_token_probs is defined)\n",
    "                if torch.isnan(next_token_probs).any() or torch.isinf(next_token_probs).any():\n",
    "                    print(\"Invalid probabilities detected!\")\n",
    "                    print(f\"Min prob: {next_token_probs.min()}, Max prob: {next_token_probs.max()}\")\n",
    "                    print(f\"Contains NaN: {torch.isnan(next_token_probs).any()}\")\n",
    "                    print(f\"Contains Inf: {torch.isinf(next_token_probs).any()}\")\n",
    "                    \n",
    "                next_token = _sample_top_p(next_token_logits, top_p)  # Note: passing logits, not probs\n",
    "            else:\n",
    "                \n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            assert next_token.size() == (1, 1)\n",
    "           \n",
    "            next_token = next_token.squeeze(0)\n",
    "            generated_tokens.append(next_token)\n",
    "            \n",
    "            if next_token.item() == self.tokenizer.eos_token_id and early_stopping:\n",
    "                break\n",
    "            \n",
    "            generation_input_ids = next_token.unsqueeze(-1)\n",
    "            kv_cache_seq_len = kv_cache.get_seq_length()\n",
    "            generation_attention_mask = torch.ones(\n",
    "                (1, kv_cache_seq_len + 1), device=next_token.device, dtype=torch.long\n",
    "            )\n",
    "            generation_cache_position = torch.tensor(\n",
    "                [next_token_pos], device=next_token.device\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(\n",
    "                    input_ids=generation_input_ids,\n",
    "                    attention_mask=generation_attention_mask,\n",
    "                    cache_position=generation_cache_position,\n",
    "                    use_cache=True,\n",
    "                    past_key_values=kv_cache,\n",
    "                )\n",
    "            logits = outputs.logits\n",
    "            next_token_pos += 1\n",
    "\n",
    "        generated_tokens = torch.cat(generated_tokens, dim=-1)\n",
    "        decoded = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        return decoded\n",
    "\n",
    "    def prefill_text_with_kv_cache(self, text: str, kv_cache: Cache):\n",
    "        # Tokenize the text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(self.model.device)\n",
    "        seq_len = input_ids.size(1)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.model.device)\n",
    "\n",
    "        if kv_cache.get_seq_length() > 0:\n",
    "            kv_cache_seq_len = kv_cache.get_seq_length()\n",
    "            attention_mask = torch.cat(\n",
    "                [\n",
    "                    torch.ones(\n",
    "                        attention_mask.shape[0],\n",
    "                        kv_cache_seq_len,\n",
    "                        dtype=attention_mask.dtype,\n",
    "                        device=attention_mask.device,\n",
    "                    ),\n",
    "                    attention_mask,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # Generate the cache positions for the tokens to be prefilled\n",
    "        cache_positions = torch.arange(\n",
    "            kv_cache.get_seq_length(), kv_cache.get_seq_length() + seq_len\n",
    "        ).to(self.model.device)\n",
    "        outputs = self._prefill_tokens(input_ids, attention_mask, cache_positions, kv_cache)\n",
    "        return kv_cache.get_seq_length(), seq_len, outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2a01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65016849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_reward(margins, query, supporting_facts, device):\n",
    "#     rewards = []\n",
    "#     for margin in margins:\n",
    "#         coherence_score = min(0.3, 0.3 * sum(1 for word in margin.split() if len(word)> 2) / max(1, len(margin.split())))\n",
    "#         relevance_score = 0.0\n",
    "#         query_terms = set(query.lower().split())\n",
    "#         margin_words = set(margin.lower().split())\n",
    "#         supporting_facts_terms = set(supporting_facts.lower().split())\n",
    "\n",
    "#         term_counts = {}\n",
    "#         for term in query_terms:\n",
    "#             term_counts[term] = sum(1 for word in margin_words if term in word)\n",
    "        \n",
    "#         if sum(term_counts.values()) > 0:\n",
    "#             relevance_score = min(0.4, sum(term_counts.values()) / (len(query_terms) * 2) * 0.4)\n",
    "\n",
    "#         # Length penalty to discourage extremely short responses\n",
    "#         length_penalty = max(0.0, min(0.2, (len(margin) / 200) * 0.2))\n",
    "        \n",
    "#         # Base reward can be smaller to enhance differentiation\n",
    "#         base_reward = 0.1\n",
    "\n",
    "#         contains_supporting_facts = any(fact.lower() in margin.lower() for fact in supporting_facts)\n",
    "#         penalty = -0.5 if not contains_supporting_facts else 1\n",
    "        \n",
    "#         reward = base_reward + coherence_score + relevance_score + length_penalty + penalty\n",
    "#         rewards.append(reward)\n",
    "#         return torch.tensor(rewards, device=device)\n",
    "    \n",
    "\n",
    "import re\n",
    "import torch\n",
    "\n",
    "def compute_reward(margins, query, supporting_facts, device):\n",
    "    \"\"\"\n",
    "    margins:      List[str]   — the generated margins to score\n",
    "    query:        str         — the query string\n",
    "    supporting_facts: List[str] — list of ground-truth supporting facts\n",
    "    device:       torch.device\n",
    "\n",
    "    Returns: FloatTensor of shape (len(margins),)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    # Pre-tokenize once\n",
    "    query_terms = set(re.findall(r'\\w+', query.lower()))\n",
    "    sf_terms = set()\n",
    "    for fact in supporting_facts:\n",
    "        sf_terms |= set(re.findall(r'\\w+', fact.lower()))\n",
    "    num_query_terms = max(1, len(query_terms))\n",
    "    num_sf_terms    = max(1, len(sf_terms))\n",
    "\n",
    "    for margin in margins:\n",
    "        # tokenize margin\n",
    "        words = re.findall(r'\\w+', margin.lower())\n",
    "        num_words = len(words)\n",
    "\n",
    "        # 1) Coherence: fraction of “real” words (length>2), capped at 0.3\n",
    "        if num_words > 0:\n",
    "            lam = sum(1 for w in words if len(w) > 2) / num_words\n",
    "        else:\n",
    "            lam = 0.0\n",
    "        coherence_score = 0.3 * lam\n",
    "\n",
    "        # 2) Query relevance: fraction of query terms present, capped at 0.3\n",
    "        present_q = sum(1 for t in query_terms if t in words)\n",
    "        relevance_score = 0.3 * min(1.0, present_q / num_query_terms)\n",
    "\n",
    "        # 3) Supporting‐fact coverage: fraction of SF tokens present, capped at 0.3\n",
    "        present_sf = sum(1 for t in sf_terms if t in words)\n",
    "        sf_score   = 0.3 * min(1.0, present_sf / num_sf_terms)\n",
    "\n",
    "        # 4) Length bonus: longer margins up to 200 tokens get up to 0.1\n",
    "        length_bonus = min(0.1, 0.1 * num_words / 200)\n",
    "\n",
    "        # 5) Base floor\n",
    "        base_reward = 0.05\n",
    "\n",
    "        # Final reward in [0.05, 1.0]\n",
    "        reward = base_reward \\\n",
    "               + coherence_score \\\n",
    "               + relevance_score \\\n",
    "               + sf_score \\\n",
    "               + length_bonus\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return torch.tensor(rewards, device=device, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520a1526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def chunk_text_to_segments(text, min_tokens_segment, tokenizer=None):\n",
    "        \"\"\"Chunk text into segments of approximately min_tokens_segment tokens.\"\"\"\n",
    "        \n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided to _chunk_text_to_segments\")\n",
    "\n",
    "        tokenizer = tokenizer\n",
    "        segments = []\n",
    "        current_segment = \"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        curr_tokens = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sent_tokens = len(tokenizer.encode(sentence))\n",
    "            \n",
    "            if curr_tokens + sent_tokens > min_tokens_segment:\n",
    "                if current_segment.strip():\n",
    "                    segments.append(current_segment.strip())\n",
    "                current_segment = sentence + \" \"\n",
    "                curr_tokens = sent_tokens\n",
    "            else:\n",
    "                current_segment += sentence + \" \"\n",
    "                curr_tokens += sent_tokens\n",
    "        \n",
    "        if current_segment.strip():\n",
    "            segments.append(current_segment.strip())\n",
    "        \n",
    "        return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df02a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RLConfig:\n",
    "    \"\"\"Configuration for the RL-based margin generation.\"\"\"\n",
    "    learning_rate: float = 5e-5\n",
    "    kl_coef: float = 0.05\n",
    "    discount_factor: float = 0.99\n",
    "    ppo_epochs: int = 5\n",
    "    ppo_mini_batch_size: int = 4\n",
    "    max_grad_norm: float = 1\n",
    "    clip_param: float = 0.1\n",
    "    value_loss_coef: float = 0.5\n",
    "    entropy_coef: float = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48ffdfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLMargin_Generator:\n",
    "    def __init__(self, model_id, rl_config: RLConfig, device='cuda', tokenizer=None):\n",
    "        self.device = device\n",
    "        self.policy_model = model_id\n",
    "        self.ref_model = model_id\n",
    "        self.tokenizer = tokenizer\n",
    "        self.avg_reward_computation = 0\n",
    "        self.count = 0\n",
    "        self.counter = 0\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        # self.policy_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",attn_implementation='eager', torch_dtype='bfloat16')\n",
    "        # self.ref_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",attn_implementation='eager', torch_dtype='bfloat16')\n",
    "        self.rl_config = rl_config\n",
    "\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.policy_model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.policy_model.parameters(),\n",
    "            lr = self.rl_config.learning_rate,\n",
    "        )\n",
    "        if torch.isnan(torch.nn.utils.clip_grad_norm_(\n",
    "            self.policy_model.parameters(),\n",
    "            self.rl_config.max_grad_norm\n",
    "        )):\n",
    "            print(\"Warning: Nan gradiesnts detected\")\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='max', factor=0.5, patience=2, verbose=2\n",
    "        )\n",
    "        \n",
    "        # Initialize the WIM inference\n",
    "        self.wim = WIMInference(self.policy_model, self.tokenizer)\n",
    "\n",
    "        self.best_avg_reward = 0\n",
    "        self.no_improvement_count = 0\n",
    "        self.patience = 3\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def _compute_kl_divergence(self, input_ids, attention_mask, labels):\n",
    "        \"\"\"Compute KL divergence between policy and reference model with better numerical stability.\"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        vocab_size = self.policy_model.config.vocab_size\n",
    "        chunk_size = 128\n",
    "\n",
    "        kl_divs = []\n",
    "        for start in range(0, seq_len - 1, chunk_size):\n",
    "            end = min(start + chunk_size, seq_len - 1)\n",
    "\n",
    "            chunk_ids   = input_ids[:, start : end+1]\n",
    "            chunk_mask  = attention_mask[:, start : end+1]\n",
    "\n",
    "            # New policy logits\n",
    "            policy_logits = self.policy_model(\n",
    "                input_ids=chunk_ids,\n",
    "                attention_mask=chunk_mask\n",
    "            ).logits[:, :-1]                   \n",
    "            # Old policy logits (frozen)\n",
    "            with torch.no_grad():\n",
    "                ref_logits = self.ref_model(\n",
    "                    input_ids=chunk_ids,\n",
    "                    attention_mask=chunk_mask\n",
    "                ).logits[:, :-1]\n",
    "\n",
    "            flat_pol = policy_logits.reshape(-1, vocab_size)\n",
    "            flat_ref = ref_logits.reshape(-1, vocab_size)\n",
    "\n",
    "            dist_new = Categorical(logits=flat_pol)\n",
    "            dist_old = Categorical(logits=flat_ref)\n",
    "            kl_flat  = kl_divergence(dist_new, dist_old)    \n",
    "\n",
    "            chunk_len = end - start\n",
    "            kl_tokens = kl_flat.reshape(batch_size, chunk_len)\n",
    "\n",
    "            # Mask out non-generated tokens\n",
    "            # labels are offset by +1 compared to logits\n",
    "            chunk_labels = labels[:, start+1 : end+1]      # (batch, chunk_len)\n",
    "            valid_mask   = (chunk_labels != -100).float()\n",
    "\n",
    "            # Sum & average only over valid positions\n",
    "            sum_kl   = (kl_tokens * valid_mask).sum(dim=1)\n",
    "            count    = valid_mask.sum(dim=1)\n",
    "            # avoid div-by-zero\n",
    "            count    = torch.where(count > 0, count, torch.ones_like(count))\n",
    "            chunk_kl = sum_kl / count\n",
    "\n",
    "            kl_divs.append(chunk_kl)\n",
    "\n",
    "        # Final per-example KL is the mean over all chunks\n",
    "        if kl_divs:\n",
    "            kl_div = torch.stack(kl_divs, dim=1).mean(dim=1)  # (batch,)\n",
    "        else:\n",
    "            kl_div = torch.zeros(batch_size, device=self.device)\n",
    "\n",
    "        return kl_div\n",
    "\n",
    "    def _validate_and_clean_margin(self, margin: str) -> str:\n",
    "        \"\"\"Validate and clean the generated margin.\"\"\"\n",
    "        # Remove excessive strange characters\n",
    "        strange_chars = \"�'`@#$%^*\"\n",
    "        for char in strange_chars:\n",
    "            margin = margin.replace(char, \"\")\n",
    "        \n",
    "        # Trim to first sentence if too long\n",
    "        if len(margin) > 200 and \".\" in margin:\n",
    "            first_part = margin.split(\".\")[0] + \".\"\n",
    "            if len(first_part) > 50:  # Ensure we have a meaningful sentence\n",
    "                margin = first_part\n",
    "        \n",
    "        # Ensure minimum length\n",
    "        if len(margin.strip()) < 10:\n",
    "            margin = \"The text does not contain sufficient information related to the query.\"\n",
    "            \n",
    "        return margin\n",
    "    \n",
    "    def save_model_output(self, output_dir: str):\n",
    "        \"\"\"Save the trained model.\"\"\"\n",
    "        self.policy_model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    def generate_rl_margin(\n",
    "        self,\n",
    "        segment,\n",
    "        query,\n",
    "        extractive_summary,\n",
    "        supporting_facts,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        early_stopping=True,\n",
    "        remove_segment=True,\n",
    "        train=False\n",
    "    ):\n",
    "        if train:\n",
    "            segments = chunk_text_to_segments(text = segment, min_tokens_segment=512, tokenizer=self.tokenizer)\n",
    "            for segment in segments:\n",
    "\n",
    "                initial_kv_cache_size = len(self.wim.wim_kv_cache)\n",
    "\n",
    "                prefilled_tokens_before_extractive_summary, _, _ = self.wim.prefill_text_with_kv_cache(\n",
    "                    segment, self.wim.wim_kv_cache\n",
    "                )\n",
    "                _, _, extractive_summary_outputs = self.wim.prefill_text_with_kv_cache(\n",
    "                    extractive_summary.format(query=query), self.wim.wim_kv_cache\n",
    "                )\n",
    "                margin = self.wim.generate_text_with_kv_cache(\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    previous_logits=extractive_summary_outputs[\"logits\"],\n",
    "                    do_sample=do_sample,\n",
    "                    top_p=top_p,\n",
    "                    temperature=temperature,\n",
    "                    early_stopping=early_stopping,\n",
    "                    kv_cache=self.wim.wim_kv_cache,\n",
    "                )\n",
    "                margin = self._validate_and_clean_margin(margin)\n",
    "                # Shrink the KV cache back to before the extractive summary prompt\n",
    "                \n",
    "                if(remove_segment):\n",
    "                    self.wim.shrink_kv_cache_from_end(new_size=initial_kv_cache_size,kv_cache=self.wim.wim_kv_cache)\n",
    "                else:\n",
    "                    self.wim.shrink_kv_cache_from_end(new_size=prefilled_tokens_before_extractive_summary,kv_cache=self.wim.wim_kv_cache,)\n",
    "\n",
    "\n",
    "                return margin\n",
    "        else:\n",
    "            initial_kv_cache_size = len(self.wim.wim_kv_cache)\n",
    "\n",
    "            prefilled_tokens_before_extractive_summary, _, _ = self.wim.prefill_text_with_kv_cache(\n",
    "                segment, self.wim.wim_kv_cache\n",
    "            )\n",
    "            _, _, extractive_summary_outputs = self.wim.prefill_text_with_kv_cache(\n",
    "                extractive_summary.format(query=query), self.wim.wim_kv_cache\n",
    "            )\n",
    "            margin = self.wim.generate_text_with_kv_cache(\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                previous_logits=extractive_summary_outputs[\"logits\"],\n",
    "                do_sample=do_sample,\n",
    "                top_p=top_p,\n",
    "                temperature=temperature,\n",
    "                early_stopping=early_stopping,\n",
    "                kv_cache=self.wim.wim_kv_cache,\n",
    "            )\n",
    "            margin = self._validate_and_clean_margin(margin)\n",
    "            # Shrink the KV cache back to before the extractive summary prompt\n",
    "            \n",
    "            if(remove_segment):\n",
    "                self.wim.shrink_kv_cache_from_end(new_size=initial_kv_cache_size,kv_cache=self.wim.wim_kv_cache)\n",
    "            else:\n",
    "                self.wim.shrink_kv_cache_from_end(new_size=prefilled_tokens_before_extractive_summary,kv_cache=self.wim.wim_kv_cache,)\n",
    "\n",
    "            # reward = compute_reward(margin, query, supporting_facts, self.device)\n",
    "            return margin\n",
    "\n",
    "\n",
    "    def train(\n",
    "            self,\n",
    "            segments, \n",
    "            query, \n",
    "            supporting_facts,\n",
    "            extractive_summary_prompt, \n",
    "            num_episodes, \n",
    "            max_new_tokens, \n",
    "            min_tokens_segment\n",
    "        ):\n",
    "        self.best_avg_reward = -float('inf')\n",
    "        self.no_improvement_count = 0\n",
    "        self.total_steps = 0\n",
    "\n",
    "        self.policy_model.train()\n",
    "        for episode in range(num_episodes):\n",
    "            # if needed to change after certain episodes\n",
    "            # if(episodes % 10 == 0):\n",
    "            self.ref_model = copy.deepcopy(self.policy_model)\n",
    "            print(f\"\\n--- Episode {episode+1}/{num_episodes} ---\")\n",
    "            # Sample a batch of segments\n",
    "            chunked_segments = chunk_text_to_segments(text = segments, min_tokens_segment=min_tokens_segment, tokenizer=self.tokenizer)\n",
    "            # print(f\"Processing {len(chunked_segments)} segments...\")\n",
    "            # Generate margins using the current policy\n",
    "            margins = []\n",
    "            # is_relevant_list = []\n",
    "            \n",
    "            for segment in tqdm(chunked_segments, desc=\"Generating margins\"):\n",
    "\n",
    "                self.wim.shrink_kv_cache_from_end(new_size=0, kv_cache=self.wim.wim_kv_cache)\n",
    "\n",
    "                margin = self.generate_rl_margin(\n",
    "                    segment=segment,\n",
    "                    query=query,\n",
    "                    extractive_summary=extractive_summary_prompt,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    train=True,\n",
    "                    supporting_facts=None\n",
    "                )\n",
    "                # print('segment:  ',segment)\n",
    "                # print('margin: ',margin)\n",
    "                margins.append(margin) \n",
    "                # torch.cuda.empty_cache()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.avg_reward_computation += self.avg_reward_computation\n",
    "                self.count += 1\n",
    "                rewards = compute_reward(margins, query, supporting_facts, self.device)\n",
    "                # Ensure rewards is a tensor on the correct device\n",
    "                if not isinstance(rewards, torch.Tensor):\n",
    "                    rewards = torch.tensor(rewards, device=self.device, dtype=torch.float32)\n",
    "                elif rewards.device != self.device:\n",
    "                    rewards = rewards.to(self.device)\n",
    "\n",
    "            inputs = []\n",
    "            for segment, margin, reward in zip(chunked_segments, margins, rewards):\n",
    "                print(f\"Query: {query}\")\n",
    "                print(f\"Segment: {segment}\")\n",
    "                print(f\"Margin: {margin}\")\n",
    "                print(f\"Reward: {reward.item()}\")\n",
    "                context = segment + extractive_summary_prompt.format(query=query) + margin\n",
    "                input_tokens = self.tokenizer(context, return_tensors=\"pt\", padding=False).to(self.device)\n",
    "                labels = input_tokens.input_ids.clone()\n",
    "                context_without_margin = segment + extractive_summary_prompt.format(query=query)\n",
    "                context_tokens = len(self.tokenizer(context_without_margin, return_tensors=\"pt\").input_ids[0])\n",
    "                labels[:, :context_tokens] = -100  \n",
    "                \n",
    "                inputs.append({\n",
    "                    \"input_ids\": input_tokens.input_ids,\n",
    "                    \"attention_mask\": input_tokens.attention_mask,\n",
    "                    \"labels\": labels,\n",
    "                })\n",
    "\n",
    "            # PPO update\n",
    "            \n",
    "            for ppo_epoch in range(self.rl_config.ppo_epochs):\n",
    "                self.counter += 1\n",
    "                if(self.counter >1000):\n",
    "                    break\n",
    "                print(f\"ppo epoch: {ppo_epoch}\")\n",
    "                total_ppo_loss = 0.0\n",
    "                total_kl_div = 0.0\n",
    "                num_samples_processed = 0\n",
    "                for i in range(len(inputs)):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    outputs = self.policy_model(\n",
    "                        input_ids=inputs[i][\"input_ids\"],\n",
    "                        attention_mask=inputs[i][\"attention_mask\"],\n",
    "                        labels=inputs[i][\"labels\"]\n",
    "                    )\n",
    "                    print('outputs.loss',outputs.loss)\n",
    "                    \n",
    "                    model_loss = outputs.loss\n",
    "                    \n",
    "                    # KL divergence\n",
    "                    kl_div = self._compute_kl_divergence(\n",
    "                        inputs[i][\"input_ids\"],\n",
    "                        inputs[i][\"attention_mask\"],\n",
    "                        inputs[i][\"labels\"],\n",
    "                    )\n",
    "                    \n",
    "                    # policy loss \n",
    "                    reward_term = rewards[i].detach() * -1.0  # Negative since we want to maximize reward\n",
    "                    kl_term = self.rl_config.kl_coef * kl_div.detach()\n",
    "                    \n",
    "                    policy_loss = model_loss + reward_term + kl_term\n",
    "                    \n",
    "                    policy_loss.backward()\n",
    "                    \n",
    "                    # Clip gradients\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.policy_model.parameters(),\n",
    "                        self.rl_config.max_grad_norm,\n",
    "                    )\n",
    "                    \n",
    "                    # Update policy model\n",
    "                    self.optimizer.step()\n",
    "                    total_ppo_loss += policy_loss.item()\n",
    "                    total_kl_div += kl_div.item() # Use .item() for logging scalars\n",
    "                    num_samples_processed += 1\n",
    "                    \n",
    "                if num_samples_processed > 0:\n",
    "                    avg_ppo_loss = total_ppo_loss / num_samples_processed\n",
    "                    avg_kl_div = total_kl_div / num_samples_processed\n",
    "                    # print(f\"  PPO Epoch {ppo_epoch+1}/{self.rl_config.ppo_epochs} - Avg Loss: {avg_ppo_loss:.4f}, Avg KL: {avg_kl_div:.4f}\")\n",
    "                # else:\n",
    "                #     print(f\"  PPO Epoch {ppo_epoch+1}/{self.rl_config.ppo_epochs} - No samples processed.\")\n",
    "\n",
    "            # === Logging & Control Flow ===\n",
    "            # Ensure rewards tensor is on CPU for numpy conversion if needed, and calculate mean\n",
    "            avg_reward = rewards.mean().item() if rewards.numel() > 0 else 0.0\n",
    "            # relevance_rate = sum(is_relevant_list) / len(is_relevant_list) # If is_relevant_list was computed\n",
    "\n",
    "            print(f\"Episode {episode+1} Summary:\")\n",
    "            print(f\"  Average reward: {avg_reward:.4f}\")\n",
    "            # print(f\"  Average KL divergence (last epoch): {avg_kl_div:.4f}\") # KL reported per PPO epoch now\n",
    "            # print(f\"Relevance rate: {relevance_rate:.4f}\") # If computed\n",
    "\n",
    "            # Update LR Scheduler\n",
    "            if self.scheduler:\n",
    "                 # Check scheduler type - some step based on loss, some on metric\n",
    "                 if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                     self.scheduler.step(avg_reward)\n",
    "                 else:\n",
    "                     # For other schedulers like StepLR, CosineAnnealingLR, etc.\n",
    "                     # Check if they need a metric or just step()\n",
    "                     try:\n",
    "                         self.scheduler.step()\n",
    "                     except TypeError:\n",
    "                         print(\"Warning: Scheduler might need a metric but received none.\")\n",
    "\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_reward > self.best_avg_reward:\n",
    "                print(f\"  New best average reward: {avg_reward:.4f} (Improvement)\")\n",
    "                self.best_avg_reward = avg_reward\n",
    "                self.no_improvement_count = 0\n",
    "                # Save best model\n",
    "                self.save_model_output(f\"best_model_episode_{episode+1}\") # Assuming this method exists\n",
    "            else:\n",
    "                self.no_improvement_count += 1\n",
    "                print(f\"  No improvement in average reward for {self.no_improvement_count} episode(s).\")\n",
    "\n",
    "            if self.no_improvement_count >= self.patience:\n",
    "                print(f\"No improvement for {self.patience} episodes. Stopping early.\")\n",
    "                break\n",
    "\n",
    "            # <<< Efficiency Recommendation >>>\n",
    "            # Clearing cache once per episode is much more reasonable than in inner loops.\n",
    "            # Do this only if memory accumulation across episodes is observed.\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "    def compute_avg_reward(self):\n",
    "        return self.avg_reward_computation / self.count if self.count > 0 else 0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54624b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_extractive_summary = \"\"\"\n",
    "```\n",
    "Given the above context, extract all information relevant to the query: \"{query}\". If the context is not relevant to the query, answer \"I don't know.\"\n",
    "{generation_header}\n",
    "\"\"\".strip()\n",
    "\n",
    "template_system_message = \"\"\"\n",
    "{user_header}\n",
    "```\n",
    "\"\"\".strip()\n",
    "\n",
    "template_final_answer = \"\"\"\n",
    "```\n",
    "{margins}\n",
    "{query}\n",
    "{generation_header}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Replace special tokens\n",
    "special_tokens = {\n",
    "    \"{user_header}\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    \"{generation_header}\": \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "}\n",
    "\n",
    "for token, replacement in special_tokens.items():\n",
    "    template_extractive_summary = template_extractive_summary.replace(token, replacement)\n",
    "    template_system_message = template_system_message.replace(token, replacement)\n",
    "    template_final_answer = template_final_answer.replace(token, replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca538313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token here\n",
    "my_token = \"\"\n",
    "login(token=my_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68d8c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_data_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    extracted_samples = []\n",
    "    for sample in data:\n",
    "\n",
    "        question = sample.get('question')\n",
    "        answer = sample.get('answer')\n",
    "        supporting_facts = sample.get('supporting_facts', [])\n",
    "        context = sample.get('context', [])\n",
    "\n",
    "        single_line_context_list = [' '.join(article[1]) for article in context]\n",
    "        # Join all context strings into one full segment.\n",
    "        segment = ' '.join(single_line_context_list)\n",
    "        \n",
    "        single_line_supporting_facts = ' '.join(fact[0] for fact in supporting_facts)\n",
    "\n",
    "        extracted_samples.append({\n",
    "            'query': question,\n",
    "            'segment': segment,\n",
    "            'supporting_facts': single_line_supporting_facts,\n",
    "            'answer' : answer\n",
    "        })\n",
    "\n",
    "    return extracted_samples\n",
    "\n",
    "# training_data = extract_data_from_json('/kaggle/input/win-data/hotpot_dev_distractor_v1.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4b3e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainaing_data = training_data[:5924]\n",
    "# test_data = training_data[5925:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d21fb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training_data[0]\n",
    "# {'query': 'Were Scott Derrickson and Ed Wood of the same nationality?',\n",
    "#  'segment': 'Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.  The film concerns the period in Wood\\'s life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau.  Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast. Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.  He lives in Los Angeles, California.  He is best known for directing horror films such as \"Sinister\", \"The Exorcism of Emily Rose\", and \"Deliver Us From Evil\", as well as the 2016 Marvel Cinematic Universe installment, \"Doctor Strange.\" Woodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States.  Its population was 403 at the 2010 census.  It is part of the Little Rock–North Little Rock–Conway Metropolitan Statistical Area.  Woodson and its accompanying Woodson Lake and Wood Hollow are the namesake for Ed Wood Sr., a prominent plantation owner, trader, and businessman at the turn of the 20th century.  Woodson is adjacent to the Wood Plantation, the largest of the plantations own by Ed Wood Sr. Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games.  Much of his work is in the action and horror film genres, with films like \"Dawn of the Dead, 300, Sucker Punch,\" and \"John Wick.\"  He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn.  With Gunn, he has scored every one of the director\\'s films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel.  In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \"The Pale Emperor\" and \"Heaven Upside Down\". Edward Davis Wood Jr. (October 10, 1924 – December 10, 1978) was an American filmmaker, actor, writer, producer, and director. Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer.  The film is officially based on a 2001 non-fiction book entitled \"Beware the Night\" by Ralph Sarchie and Lisa Collier Cool, and its marketing campaign highlighted that it was \"inspired by actual accounts\".  The film stars Eric Bana, Édgar Ramírez, Sean Harris, Olivia Munn, and Joel McHale in the main roles and was released on July 2, 2014. Adam Collis is an American filmmaker and actor.  He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010.  He also studied cinema at the University of Southern California from 1991 to 1997.  Collis first work was the assistant director for the Scott Derrickson\\'s short \"Love in the Ruins\" (1995).  In 1998, he played \"Crankshaft\" in Eric Koyanagi\\'s \"Hundred Percent\". Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill.  It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger. Conrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American actor.  He moved to Hollywood, California in 1948 to pursue a career in acting.  He got his start in movies appearing in Ed Wood films such as \"Plan 9 from Outer Space\", \"Glen or Glenda\", and \"Jail Bait.\"  He took a break from acting during the 1960s and 1970s but due to the ongoing interest in the films of Ed Wood, he reemerged in the 1980s and has become a prolific actor.  He also has since gone on to write, produce and direct several films. Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures.  It is the fourteenth film of the Marvel Cinematic Universe (MCU).  The film was directed by Scott Derrickson, who wrote it with Jon Spaihts and C. Robert Cargill, and stars Benedict Cumberbatch as Stephen Strange, along with Chiwetel Ejiofor, Rachel McAdams, Benedict Wong, Michael Stuhlbarg, Benjamin Bratt, Scott Adkins, Mads Mikkelsen, and Tilda Swinton.  In \"Doctor Strange\", surgeon Strange learns the mystic arts after a career-ending car accident.',\n",
    "#  'supporting_facts': 'Scott Derrickson 0 Ed Wood 0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "115e212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "all_samples = extract_data_from_json('/kaggle/input/wim-data/hotpot_dev_distractor_v1.json')\n",
    "train_samples, val_samples = train_test_split(all_samples, test_size=0.2, random_state=42)\n",
    "\n",
    "class HotpotDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "def collate_batch(batch):\n",
    "    batch_queries = [sample['query'] for sample in batch]\n",
    "    batch_segments = [sample['segment'] for sample in batch]\n",
    "    batch_supporting_facts = [sample['supporting_facts'] for sample in batch]\n",
    "    batch_answer = [sample['answer'] for sample in batch]\n",
    "    return {\n",
    "        'query': batch_queries,\n",
    "        'segment': batch_segments,\n",
    "        'supporting_facts': batch_supporting_facts,\n",
    "        'answer':batch_answer\n",
    "    }\n",
    "\n",
    "train_ds = HotpotDataset(train_samples)\n",
    "val_ds = HotpotDataset(val_samples)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=8, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=8, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f075a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_extractive_summary = \"\"\"\n",
    "```\n",
    "Given the above context, extract all information relevant to the query: \"{query}\". If the context is not relevant to the query, answer \"I don't know.\"\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30248abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episode 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 7/7 [00:06<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Donkey Kong Country Returns is a side-scrolling platformer video game developed by Retro Studios and published by Nintendo for the Wii console. The game was released first in North America in November 2010, and in PAL regions and Japan the following month. A stereoscopic port of the game, titled Donkey Kong Country Returns 3D, was released for the Nintendo 3DS in May 2013, and in Japan the following month. The 34th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA), honored the best in film for 2008. Pixar's animated film \"WALL-E\" won the Best Film award and became the first ever animated film to do so, however, the film lost the Best Animated Film award to \"Waltz with Bashir\". \"Man! I Feel Like a Woman!\" is a song recorded by Canadian singer-songwriter Shania Twain taken from her third studio album, \"Come On Over\" (1997).\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Country Bears was released first in North America in November 2010, and in PAL regions and Japan the following month.\"\n",
      "Reward: 0.47863635420799255\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Written by Twain with her longtime collaborator and then-husband Robert John \"Mutt\" Lange, who also produced the track, the song was released first to North American country radio stations in March 1999 as the seventh single from the album, and later it was released worldwide in September 1999. \"Man! I Feel Like a Woman!\" is a country pop song with lyrics about female empowerment. John Otway & Wild Willy Barrett is the 1976 debut album by English folk singer-songwriter duo John Otway and Wild Willy Barrett. Released first on their own Extracked Records, the album is a collection of recordings made between 1971 and 1976. The Country Bears is a 2002 American family musical comedy film, directed by Peter Hastings, produced by Walt Disney Pictures, and based on the Disney theme park attraction \"Country Bear Jamboree\". The film stars Haley Joel Osment as the voice of Beary Barrington with supporting roles done by Christopher Walken, Stephen Tobolowsky, Daryl Mitchell, M.C.\n",
      "Margin: \n",
      "Answer: The Country Bears was released first in 1976, while The Wild was released in 2002.\n",
      "Reward: 0.5114091038703918\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Gainey, Diedrich Bader, Alex Rocco, Meagen Fay, Eli Marienthal, and the voice talents of Diedrich Bader, Candy Ford, James Gammon, Brad Garrett, Toby Huss, Kevin Michael Richardson, and Stephen Root. Candy Ford (born May 1, 1975) is an American comedian and television actress, best known for starring in the sketch comedy, \"The Rerun Show\", Ford has also appeared in other TV programs including: \"Curb Your Enthusiasm\", \"Will & Grace\", and she provided voicework for the \"Law & Order\" videogame, and starred on the short-lived NBC sketch comedy, \"The Rerun Show\" and voiced Trixie in the film \"The Country Bears\" and later starred in \"Girls Behaving Badly\".\n",
      "Margin: \n",
      "Answer: Based on the context, \"The Wild\" was released first, followed by \"The Country Bears\".\n",
      "Reward: 0.5084090828895569\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Ogu and Mampato in Rapa Nui (Spanish: \"Ogú y Mampato en Rapa Nui\" ), also known as Mampato: The Movie (Spanish: \"Mampato: La Película\") is a feature-length Chilean animated film, created by Cine Animadores and executive produced by Elastic Studios, released June 27, 2002. Although the film isn't the first animated feature made in Chile, being the second after Alfredo Serey's 1921 film \"La Trasmisión del Mando Presidencial\" (\"The Transmission of Presidential Control\"), it is considered the country's first \"modern\" animated film. The movie is based on the Chilean comics character Mampato created in 1971 for the magazine of the same name by Themo Lobos and Eduardo Armstrong, and later reprinted as the comic-book Cucalón, the story for the film being adapted from the seventh adventure in the series: \"Mata-ki-te-rangui\".\n",
      "Margin: .\n",
      "The country where the film was released is Chile, and the film is \"The Country Bears\". Therefore, the answer to the question is \"The Country Bears\".\n",
      "Reward: 0.4804825186729431\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: The Wild is a 2006 American 3D computer animated comedy directed by animator Steve \"Spaz\" Williams, and written by Ed Decter, John J. Strauss, Mark Gibson and Philip Halprin. It features the voices of Kiefer Sutherland, Jim Belushi, Janeane Garofalo, Greg Cipes, Eddie Izzard, Richard Kind, William Shatner and Patrick Warburton. Mickey's House of Villains (also known as House of Mouse: The Villains) is a 2002 direct-to-video animated film produced by The Walt Disney Company (Walt Disney Television Animation and Toon City Animation, with animation coordination by Walt Disney Feature Animation Florida. It is based on the Disney Channel animated television series \"Disney's House of Mouse\" and a sequel to the direct-to-video animated film \"\", starring Mickey Mouse, Donald Duck, Minnie Mouse, Goofy, Daisy Duck and Disney Villains that have appeared in past Disney productions. It was released on both VHS and DVD by Walt Disney Home Video on September 3, 2002.\n",
      "Margin: .\n",
      "Answer: The Country Bears and The Wild were both animated films released in 2006.\n",
      "\n",
      "The Country Bears was released on November 20, 2006, and is a childrens\n",
      "Reward: 0.5176035165786743\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: It was followed by a 2004 direct-to-video animated film, \"\", produced by DisneyToon Studios, on August 17, 2004. Toys in the Attic (Czech: Na půdě aneb Kdo má dneska narozeniny? ; festival title: In the Attic: Who Has a Birthday Today?) is a 2009 Czech-French-Japanese-Slovak primarily stop-motion animated fantasy comedy thriller family film directed by Jiří Barta and written by Edgar Dutka and Barta which depicts a community of toys and other objects in an attic who come to life when no human is around. It is an international co-production of Czech, Japanese and Slovak companies. The film was released first in the Czech Republic on 5 March 2009 and has been shown subtitled at film festivals internationally.\n",
      "Margin: .\n",
      "Answer: The Country Bears and The Wild are not related to each other. The Country Bears was released first in 2004, while The Wild was released in 2009.\n",
      "Reward: 0.5227662324905396\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: An American dub – adapted, produced and directed by Vivian Schilling and performed by actors including Forest Whitaker, Joan Cusack, Cary Elwes and Schilling herself – has been recorded, which the film was first shown with on 3 March 2012 at the New York International Children's Film Festival and was released nationally on 24 August 2012 by Hannover House.\n",
      "Margin: \n",
      "The query \"Which animated film was released first, The Country Bears or The Wild?\" does not provide enough context to extract the relevant information.\n",
      "Reward: 0.6370000243186951\n",
      "ppo epoch: 0\n",
      "outputs.loss tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.1781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.1725, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.1609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.7170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.8378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.7394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 1\n",
      "outputs.loss tensor(0.7546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.7321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.5253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.3327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.0649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 2\n",
      "outputs.loss tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.3453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.3675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 3\n",
      "outputs.loss tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.8706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 4\n",
      "outputs.loss tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Episode 1 Summary:\n",
      "  Average reward: 0.5223\n",
      "  New best average reward: 0.5223 (Improvement)\n",
      "\n",
      "--- Episode 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 7/7 [00:08<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Donkey Kong Country Returns is a side-scrolling platformer video game developed by Retro Studios and published by Nintendo for the Wii console. The game was released first in North America in November 2010, and in PAL regions and Japan the following month. A stereoscopic port of the game, titled Donkey Kong Country Returns 3D, was released for the Nintendo 3DS in May 2013, and in Japan the following month. The 34th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA), honored the best in film for 2008. Pixar's animated film \"WALL-E\" won the Best Film award and became the first ever animated film to do so, however, the film lost the Best Animated Film award to \"Waltz with Bashir\". \"Man! I Feel Like a Woman!\" is a song recorded by Canadian singer-songwriter Shania Twain taken from her third studio album, \"Come On Over\" (1997).\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Country Bears was released first in North America in November 2010, and in PAL regions and Japan the following month.\n",
      "Reward: 0.47863635420799255\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Written by Twain with her longtime collaborator and then-husband Robert John \"Mutt\" Lange, who also produced the track, the song was released first to North American country radio stations in March 1999 as the seventh single from the album, and later it was released worldwide in September 1999. \"Man! I Feel Like a Woman!\" is a country pop song with lyrics about female empowerment. John Otway & Wild Willy Barrett is the 1976 debut album by English folk singer-songwriter duo John Otway and Wild Willy Barrett. Released first on their own Extracked Records, the album is a collection of recordings made between 1971 and 1976. The Country Bears is a 2002 American family musical comedy film, directed by Peter Hastings, produced by Walt Disney Pictures, and based on the Disney theme park attraction \"Country Bear Jamboree\". The film stars Haley Joel Osment as the voice of Beary Barrington with supporting roles done by Christopher Walken, Stephen Tobolowsky, Daryl Mitchell, M.C.\n",
      "Margin: \n",
      "Answer: The Country Bears was released first in 1976, while The Wild was released in 2002.\n",
      "\n",
      "The Country Bears was released first in 1976, while The Wild was\n",
      "Reward: 0.5227662324905396\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Gainey, Diedrich Bader, Alex Rocco, Meagen Fay, Eli Marienthal, and the voice talents of Diedrich Bader, Candy Ford, James Gammon, Brad Garrett, Toby Huss, Kevin Michael Richardson, and Stephen Root. Candy Ford (born May 1, 1975) is an American comedian and television actress, best known for starring in the sketch comedy, \"The Rerun Show\", Ford has also appeared in other TV programs including: \"Curb Your Enthusiasm\", \"Will & Grace\", and she provided voicework for the \"Law & Order\" videogame, and starred on the short-lived NBC sketch comedy, \"The Rerun Show\" and voiced Trixie in the film \"The Country Bears\" and later starred in \"Girls Behaving Badly\".\n",
      "Margin: \n",
      "Answer: Based on the context, \"The Wild\" was released first, followed by \"The Country Bears\".\n",
      "\n",
      "The Country Bears was released first, followed by \"The Wild\".\n",
      "\n",
      "Based on the context, \"The\n",
      "Reward: 0.5159090757369995\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Ogu and Mampato in Rapa Nui (Spanish: \"Ogú y Mampato en Rapa Nui\" ), also known as Mampato: The Movie (Spanish: \"Mampato: La Película\") is a feature-length Chilean animated film, created by Cine Animadores and executive produced by Elastic Studios, released June 27, 2002. Although the film isn't the first animated feature made in Chile, being the second after Alfredo Serey's 1921 film \"La Trasmisión del Mando Presidencial\" (\"The Transmission of Presidential Control\"), it is considered the country's first \"modern\" animated film. The movie is based on the Chilean comics character Mampato created in 1971 for the magazine of the same name by Themo Lobos and Eduardo Armstrong, and later reprinted as the comic-book Cucalón, the story for the film being adapted from the seventh adventure in the series: \"Mata-ki-te-rangui\".\n",
      "Margin: \n",
      "The country where the film was released is Chile, and the film is \"The Country Bears\".\n",
      "Reward: 0.4841363728046417\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: The Wild is a 2006 American 3D computer animated comedy directed by animator Steve \"Spaz\" Williams, and written by Ed Decter, John J. Strauss, Mark Gibson and Philip Halprin. It features the voices of Kiefer Sutherland, Jim Belushi, Janeane Garofalo, Greg Cipes, Eddie Izzard, Richard Kind, William Shatner and Patrick Warburton. Mickey's House of Villains (also known as House of Mouse: The Villains) is a 2002 direct-to-video animated film produced by The Walt Disney Company (Walt Disney Television Animation and Toon City Animation, with animation coordination by Walt Disney Feature Animation Florida. It is based on the Disney Channel animated television series \"Disney's House of Mouse\" and a sequel to the direct-to-video animated film \"\", starring Mickey Mouse, Donald Duck, Minnie Mouse, Goofy, Daisy Duck and Disney Villains that have appeared in past Disney productions. It was released on both VHS and DVD by Walt Disney Home Video on September 3, 2002.\n",
      "Margin: ..\n",
      "Answer: The Country Bears and The Wild were both animated films released in 2006..\n",
      "\n",
      "The Country Bears was released on November 20, 2006, and is a children\n",
      "Reward: 0.5176035165786743\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: It was followed by a 2004 direct-to-video animated film, \"\", produced by DisneyToon Studios, on August 17, 2004. Toys in the Attic (Czech: Na půdě aneb Kdo má dneska narozeniny? ; festival title: In the Attic: Who Has a Birthday Today?) is a 2009 Czech-French-Japanese-Slovak primarily stop-motion animated fantasy comedy thriller family film directed by Jiří Barta and written by Edgar Dutka and Barta which depicts a community of toys and other objects in an attic who come to life when no human is around. It is an international co-production of Czech, Japanese and Slovak companies. The film was released first in the Czech Republic on 5 March 2009 and has been shown subtitled at film festivals internationally.\n",
      "Margin: .. The Country Bears and The Wild are not related to each other. The Country Bears was released first in 2004, while The Wild was released in 2009...\n",
      "Answer: The Country\n",
      "Reward: 0.5259090662002563\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: An American dub – adapted, produced and directed by Vivian Schilling and performed by actors including Forest Whitaker, Joan Cusack, Cary Elwes and Schilling herself – has been recorded, which the film was first shown with on 3 March 2012 at the New York International Children's Film Festival and was released nationally on 24 August 2012 by Hannover House.\n",
      "Margin: \n",
      "The query \"Which animated film was released first, The Country Bears or The Wild?\" does not provide enough context to extract the relevant information.\n",
      "Reward: 0.6370000243186951\n",
      "ppo epoch: 0\n",
      "outputs.loss tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.7920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.8347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 1\n",
      "outputs.loss tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.5954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 2\n",
      "outputs.loss tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.3933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.3699e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 3\n",
      "outputs.loss tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.6911e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.5820e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 4\n",
      "outputs.loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.1591e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Episode 2 Summary:\n",
      "  Average reward: 0.5260\n",
      "  New best average reward: 0.5260 (Improvement)\n",
      "\n",
      "--- Episode 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 7/7 [00:07<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Donkey Kong Country Returns is a side-scrolling platformer video game developed by Retro Studios and published by Nintendo for the Wii console. The game was released first in North America in November 2010, and in PAL regions and Japan the following month. A stereoscopic port of the game, titled Donkey Kong Country Returns 3D, was released for the Nintendo 3DS in May 2013, and in Japan the following month. The 34th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA), honored the best in film for 2008. Pixar's animated film \"WALL-E\" won the Best Film award and became the first ever animated film to do so, however, the film lost the Best Animated Film award to \"Waltz with Bashir\". \"Man! I Feel Like a Woman!\" is a song recorded by Canadian singer-songwriter Shania Twain taken from her third studio album, \"Come On Over\" (1997).\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Country Bears was released first in North America in November 2010, and in PAL regions and Japan the following month.\n",
      "Reward: 0.47863635420799255\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Written by Twain with her longtime collaborator and then-husband Robert John \"Mutt\" Lange, who also produced the track, the song was released first to North American country radio stations in March 1999 as the seventh single from the album, and later it was released worldwide in September 1999. \"Man! I Feel Like a Woman!\" is a country pop song with lyrics about female empowerment. John Otway & Wild Willy Barrett is the 1976 debut album by English folk singer-songwriter duo John Otway and Wild Willy Barrett. Released first on their own Extracked Records, the album is a collection of recordings made between 1971 and 1976. The Country Bears is a 2002 American family musical comedy film, directed by Peter Hastings, produced by Walt Disney Pictures, and based on the Disney theme park attraction \"Country Bear Jamboree\". The film stars Haley Joel Osment as the voice of Beary Barrington with supporting roles done by Christopher Walken, Stephen Tobolowsky, Daryl Mitchell, M.C.\n",
      "Margin: \n",
      "Answer: The Country Bears was released first in 1976, while The Wild was released in 2002.\n",
      "\n",
      "The Country Bears was released first in 1976, while The Wild was\n",
      "Reward: 0.5227662324905396\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Gainey, Diedrich Bader, Alex Rocco, Meagen Fay, Eli Marienthal, and the voice talents of Diedrich Bader, Candy Ford, James Gammon, Brad Garrett, Toby Huss, Kevin Michael Richardson, and Stephen Root. Candy Ford (born May 1, 1975) is an American comedian and television actress, best known for starring in the sketch comedy, \"The Rerun Show\", Ford has also appeared in other TV programs including: \"Curb Your Enthusiasm\", \"Will & Grace\", and she provided voicework for the \"Law & Order\" videogame, and starred on the short-lived NBC sketch comedy, \"The Rerun Show\" and voiced Trixie in the film \"The Country Bears\" and later starred in \"Girls Behaving Badly\".\n",
      "Margin: \n",
      "Answer: Based on the context, \"The Wild\" was released first, followed by \"The Country Bears\".\n",
      "\n",
      "The Country Bears was released first, followed by \"The Wild\".\n",
      "\n",
      "The Country Bears was released first\n",
      "Reward: 0.5273768305778503\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Ogu and Mampato in Rapa Nui (Spanish: \"Ogú y Mampato en Rapa Nui\" ), also known as Mampato: The Movie (Spanish: \"Mampato: La Película\") is a feature-length Chilean animated film, created by Cine Animadores and executive produced by Elastic Studios, released June 27, 2002. Although the film isn't the first animated feature made in Chile, being the second after Alfredo Serey's 1921 film \"La Trasmisión del Mando Presidencial\" (\"The Transmission of Presidential Control\"), it is considered the country's first \"modern\" animated film. The movie is based on the Chilean comics character Mampato created in 1971 for the magazine of the same name by Themo Lobos and Eduardo Armstrong, and later reprinted as the comic-book Cucalón, the story for the film being adapted from the seventh adventure in the series: \"Mata-ki-te-rangui\".\n",
      "Margin: \n",
      "The country where the film was released is Chile, and the film is \"The Country Bears\".\n",
      "Reward: 0.4841363728046417\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: The Wild is a 2006 American 3D computer animated comedy directed by animator Steve \"Spaz\" Williams, and written by Ed Decter, John J. Strauss, Mark Gibson and Philip Halprin. It features the voices of Kiefer Sutherland, Jim Belushi, Janeane Garofalo, Greg Cipes, Eddie Izzard, Richard Kind, William Shatner and Patrick Warburton. Mickey's House of Villains (also known as House of Mouse: The Villains) is a 2002 direct-to-video animated film produced by The Walt Disney Company (Walt Disney Television Animation and Toon City Animation, with animation coordination by Walt Disney Feature Animation Florida. It is based on the Disney Channel animated television series \"Disney's House of Mouse\" and a sequel to the direct-to-video animated film \"\", starring Mickey Mouse, Donald Duck, Minnie Mouse, Goofy, Daisy Duck and Disney Villains that have appeared in past Disney productions. It was released on both VHS and DVD by Walt Disney Home Video on September 3, 2002.\n",
      "Margin: ..\n",
      "Answer: The Country Bears and The Wild were both animated films released in 2006..\n",
      "\n",
      "The Country Bears was released on November 20, 2006..\n",
      "\n",
      "The Country Bears\n",
      "Reward: 0.5192937254905701\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: It was followed by a 2004 direct-to-video animated film, \"\", produced by DisneyToon Studios, on August 17, 2004. Toys in the Attic (Czech: Na půdě aneb Kdo má dneska narozeniny? ; festival title: In the Attic: Who Has a Birthday Today?) is a 2009 Czech-French-Japanese-Slovak primarily stop-motion animated fantasy comedy thriller family film directed by Jiří Barta and written by Edgar Dutka and Barta which depicts a community of toys and other objects in an attic who come to life when no human is around. It is an international co-production of Czech, Japanese and Slovak companies. The film was released first in the Czech Republic on 5 March 2009 and has been shown subtitled at film festivals internationally.\n",
      "Margin: .. The Country Bears and The Wild are not related to each other. The Country Bears was released first in 2004...\n",
      "Answer: The Country Bears and The Wild were not related to each other. The Country B\n",
      "Reward: 0.5443257689476013\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: An American dub – adapted, produced and directed by Vivian Schilling and performed by actors including Forest Whitaker, Joan Cusack, Cary Elwes and Schilling herself – has been recorded, which the film was first shown with on 3 March 2012 at the New York International Children's Film Festival and was released nationally on 24 August 2012 by Hannover House.\n",
      "Margin: \n",
      "The query \"Which animated film was released first, The Country Bears or The Wild?\" does not provide enough context to extract the relevant information.\n",
      "Reward: 0.6370000243186951\n",
      "ppo epoch: 0\n",
      "outputs.loss tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(6.2119e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 1\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.8567e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(6.4799e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 2\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.6020e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.8725e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 3\n",
      "outputs.loss tensor(4.9470e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.5582e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.2580e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 4\n",
      "outputs.loss tensor(4.6515e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.5468e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.2510e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Episode 3 Summary:\n",
      "  Average reward: 0.5305\n",
      "  New best average reward: 0.5305 (Improvement)\n",
      "\n",
      "--- Episode 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 7/7 [00:07<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Donkey Kong Country Returns is a side-scrolling platformer video game developed by Retro Studios and published by Nintendo for the Wii console. The game was released first in North America in November 2010, and in PAL regions and Japan the following month. A stereoscopic port of the game, titled Donkey Kong Country Returns 3D, was released for the Nintendo 3DS in May 2013, and in Japan the following month. The 34th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA), honored the best in film for 2008. Pixar's animated film \"WALL-E\" won the Best Film award and became the first ever animated film to do so, however, the film lost the Best Animated Film award to \"Waltz with Bashir\". \"Man! I Feel Like a Woman!\" is a song recorded by Canadian singer-songwriter Shania Twain taken from her third studio album, \"Come On Over\" (1997).\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Country Bears was released first in North America in November 2010, and in PAL regions and Japan the following month.\n",
      "Reward: 0.47863635420799255\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Written by Twain with her longtime collaborator and then-husband Robert John \"Mutt\" Lange, who also produced the track, the song was released first to North American country radio stations in March 1999 as the seventh single from the album, and later it was released worldwide in September 1999. \"Man! I Feel Like a Woman!\" is a country pop song with lyrics about female empowerment. John Otway & Wild Willy Barrett is the 1976 debut album by English folk singer-songwriter duo John Otway and Wild Willy Barrett. Released first on their own Extracked Records, the album is a collection of recordings made between 1971 and 1976. The Country Bears is a 2002 American family musical comedy film, directed by Peter Hastings, produced by Walt Disney Pictures, and based on the Disney theme park attraction \"Country Bear Jamboree\". The film stars Haley Joel Osment as the voice of Beary Barrington with supporting roles done by Christopher Walken, Stephen Tobolowsky, Daryl Mitchell, M.C.\n",
      "Margin: \n",
      "Answer: The Country Bears was released first in 1976, while The Wild was released in 2002.\n",
      "\n",
      "The Country Bears was released first in 1976, while The Wild was\n",
      "Reward: 0.5227662324905396\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Gainey, Diedrich Bader, Alex Rocco, Meagen Fay, Eli Marienthal, and the voice talents of Diedrich Bader, Candy Ford, James Gammon, Brad Garrett, Toby Huss, Kevin Michael Richardson, and Stephen Root. Candy Ford (born May 1, 1975) is an American comedian and television actress, best known for starring in the sketch comedy, \"The Rerun Show\", Ford has also appeared in other TV programs including: \"Curb Your Enthusiasm\", \"Will & Grace\", and she provided voicework for the \"Law & Order\" videogame, and starred on the short-lived NBC sketch comedy, \"The Rerun Show\" and voiced Trixie in the film \"The Country Bears\" and later starred in \"Girls Behaving Badly\".\n",
      "Margin: \n",
      "Answer: Based on the context, \"The Wild\" was released first, followed by \"The Country Bears\".\n",
      "\n",
      "The Country Bears was released first, followed by \"The Wild\".\n",
      "\n",
      "The Country Bears was released first\n",
      "Reward: 0.5273768305778503\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Ogu and Mampato in Rapa Nui (Spanish: \"Ogú y Mampato en Rapa Nui\" ), also known as Mampato: The Movie (Spanish: \"Mampato: La Película\") is a feature-length Chilean animated film, created by Cine Animadores and executive produced by Elastic Studios, released June 27, 2002. Although the film isn't the first animated feature made in Chile, being the second after Alfredo Serey's 1921 film \"La Trasmisión del Mando Presidencial\" (\"The Transmission of Presidential Control\"), it is considered the country's first \"modern\" animated film. The movie is based on the Chilean comics character Mampato created in 1971 for the magazine of the same name by Themo Lobos and Eduardo Armstrong, and later reprinted as the comic-book Cucalón, the story for the film being adapted from the seventh adventure in the series: \"Mata-ki-te-rangui\".\n",
      "Margin: \n",
      "The country where the film was released is Chile, and the film is \"The Country Bears\".\n",
      "Reward: 0.4841363728046417\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: The Wild is a 2006 American 3D computer animated comedy directed by animator Steve \"Spaz\" Williams, and written by Ed Decter, John J. Strauss, Mark Gibson and Philip Halprin. It features the voices of Kiefer Sutherland, Jim Belushi, Janeane Garofalo, Greg Cipes, Eddie Izzard, Richard Kind, William Shatner and Patrick Warburton. Mickey's House of Villains (also known as House of Mouse: The Villains) is a 2002 direct-to-video animated film produced by The Walt Disney Company (Walt Disney Television Animation and Toon City Animation, with animation coordination by Walt Disney Feature Animation Florida. It is based on the Disney Channel animated television series \"Disney's House of Mouse\" and a sequel to the direct-to-video animated film \"\", starring Mickey Mouse, Donald Duck, Minnie Mouse, Goofy, Daisy Duck and Disney Villains that have appeared in past Disney productions. It was released on both VHS and DVD by Walt Disney Home Video on September 3, 2002.\n",
      "Margin: ..\n",
      "Answer: The Country Bears and The Wild were both animated films released in 2006..\n",
      "\n",
      "The Country Bears was released on November 20, 2006..\n",
      "\n",
      "The Country Bears\n",
      "Reward: 0.5192937254905701\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: It was followed by a 2004 direct-to-video animated film, \"\", produced by DisneyToon Studios, on August 17, 2004. Toys in the Attic (Czech: Na půdě aneb Kdo má dneska narozeniny? ; festival title: In the Attic: Who Has a Birthday Today?) is a 2009 Czech-French-Japanese-Slovak primarily stop-motion animated fantasy comedy thriller family film directed by Jiří Barta and written by Edgar Dutka and Barta which depicts a community of toys and other objects in an attic who come to life when no human is around. It is an international co-production of Czech, Japanese and Slovak companies. The film was released first in the Czech Republic on 5 March 2009 and has been shown subtitled at film festivals internationally.\n",
      "Margin: .. The Country Bears and The Wild are not related to each other. The Country Bears was released first in 2004...\n",
      "Answer: The Country Bears and The Wild were not related to each other. The Country B\n",
      "Reward: 0.5443257689476013\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: An American dub – adapted, produced and directed by Vivian Schilling and performed by actors including Forest Whitaker, Joan Cusack, Cary Elwes and Schilling herself – has been recorded, which the film was first shown with on 3 March 2012 at the New York International Children's Film Festival and was released nationally on 24 August 2012 by Hannover House.\n",
      "Margin: \n",
      "The query \"Which animated film was released first, The Country Bears or The Wild?\" does not provide enough context to extract the relevant information.\n",
      "Reward: 0.6370000243186951\n",
      "ppo epoch: 0\n",
      "outputs.loss tensor(4.2629e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.0913e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(8.7524e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.2800e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.6201e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 1\n",
      "outputs.loss tensor(1.6029e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.9418e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.2732e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.5275e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 2\n",
      "outputs.loss tensor(1.3506e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.8914e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.0406e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.1614e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.3213e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 3\n",
      "outputs.loss tensor(1.2431e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.0229e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.3933e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.1722e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.8591e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.2736e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 4\n",
      "outputs.loss tensor(1.1943e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.5920e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.2384e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.2091e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.0456e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.2927e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Episode 4 Summary:\n",
      "  Average reward: 0.5305\n",
      "  No improvement in average reward for 1 episode(s).\n",
      "\n",
      "--- Episode 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 7/7 [00:07<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Donkey Kong Country Returns is a side-scrolling platformer video game developed by Retro Studios and published by Nintendo for the Wii console. The game was released first in North America in November 2010, and in PAL regions and Japan the following month. A stereoscopic port of the game, titled Donkey Kong Country Returns 3D, was released for the Nintendo 3DS in May 2013, and in Japan the following month. The 34th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA), honored the best in film for 2008. Pixar's animated film \"WALL-E\" won the Best Film award and became the first ever animated film to do so, however, the film lost the Best Animated Film award to \"Waltz with Bashir\". \"Man! I Feel Like a Woman!\" is a song recorded by Canadian singer-songwriter Shania Twain taken from her third studio album, \"Come On Over\" (1997).\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Country Bears was released first in North America in November 2010, and in PAL regions and Japan the following month.\n",
      "Reward: 0.47863635420799255\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Written by Twain with her longtime collaborator and then-husband Robert John \"Mutt\" Lange, who also produced the track, the song was released first to North American country radio stations in March 1999 as the seventh single from the album, and later it was released worldwide in September 1999. \"Man! I Feel Like a Woman!\" is a country pop song with lyrics about female empowerment. John Otway & Wild Willy Barrett is the 1976 debut album by English folk singer-songwriter duo John Otway and Wild Willy Barrett. Released first on their own Extracked Records, the album is a collection of recordings made between 1971 and 1976. The Country Bears is a 2002 American family musical comedy film, directed by Peter Hastings, produced by Walt Disney Pictures, and based on the Disney theme park attraction \"Country Bear Jamboree\". The film stars Haley Joel Osment as the voice of Beary Barrington with supporting roles done by Christopher Walken, Stephen Tobolowsky, Daryl Mitchell, M.C.\n",
      "Margin: \n",
      "Answer: The Country Bears was released first in 1976, while The Wild was released in 2002.\n",
      "\n",
      "The Country Bears was released first in 1976, while The Wild was\n",
      "Reward: 0.5227662324905396\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Gainey, Diedrich Bader, Alex Rocco, Meagen Fay, Eli Marienthal, and the voice talents of Diedrich Bader, Candy Ford, James Gammon, Brad Garrett, Toby Huss, Kevin Michael Richardson, and Stephen Root. Candy Ford (born May 1, 1975) is an American comedian and television actress, best known for starring in the sketch comedy, \"The Rerun Show\", Ford has also appeared in other TV programs including: \"Curb Your Enthusiasm\", \"Will & Grace\", and she provided voicework for the \"Law & Order\" videogame, and starred on the short-lived NBC sketch comedy, \"The Rerun Show\" and voiced Trixie in the film \"The Country Bears\" and later starred in \"Girls Behaving Badly\".\n",
      "Margin: \n",
      "Answer: Based on the context, \"The Wild\" was released first, followed by \"The Country Bears\".\n",
      "\n",
      "The Country Bears was released first, followed by \"The Wild\".\n",
      "\n",
      "The Country Bears was released first\n",
      "Reward: 0.5273768305778503\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: Ogu and Mampato in Rapa Nui (Spanish: \"Ogú y Mampato en Rapa Nui\" ), also known as Mampato: The Movie (Spanish: \"Mampato: La Película\") is a feature-length Chilean animated film, created by Cine Animadores and executive produced by Elastic Studios, released June 27, 2002. Although the film isn't the first animated feature made in Chile, being the second after Alfredo Serey's 1921 film \"La Trasmisión del Mando Presidencial\" (\"The Transmission of Presidential Control\"), it is considered the country's first \"modern\" animated film. The movie is based on the Chilean comics character Mampato created in 1971 for the magazine of the same name by Themo Lobos and Eduardo Armstrong, and later reprinted as the comic-book Cucalón, the story for the film being adapted from the seventh adventure in the series: \"Mata-ki-te-rangui\".\n",
      "Margin: \n",
      "The country where the film was released is Chile, and the film is \"The Country Bears\".\n",
      "Reward: 0.4841363728046417\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: The Wild is a 2006 American 3D computer animated comedy directed by animator Steve \"Spaz\" Williams, and written by Ed Decter, John J. Strauss, Mark Gibson and Philip Halprin. It features the voices of Kiefer Sutherland, Jim Belushi, Janeane Garofalo, Greg Cipes, Eddie Izzard, Richard Kind, William Shatner and Patrick Warburton. Mickey's House of Villains (also known as House of Mouse: The Villains) is a 2002 direct-to-video animated film produced by The Walt Disney Company (Walt Disney Television Animation and Toon City Animation, with animation coordination by Walt Disney Feature Animation Florida. It is based on the Disney Channel animated television series \"Disney's House of Mouse\" and a sequel to the direct-to-video animated film \"\", starring Mickey Mouse, Donald Duck, Minnie Mouse, Goofy, Daisy Duck and Disney Villains that have appeared in past Disney productions. It was released on both VHS and DVD by Walt Disney Home Video on September 3, 2002.\n",
      "Margin: ..\n",
      "Answer: The Country Bears and The Wild were both animated films released in 2006..\n",
      "\n",
      "The Country Bears was released on November 20, 2006..\n",
      "\n",
      "The Country Bears\n",
      "Reward: 0.5192937254905701\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: It was followed by a 2004 direct-to-video animated film, \"\", produced by DisneyToon Studios, on August 17, 2004. Toys in the Attic (Czech: Na půdě aneb Kdo má dneska narozeniny? ; festival title: In the Attic: Who Has a Birthday Today?) is a 2009 Czech-French-Japanese-Slovak primarily stop-motion animated fantasy comedy thriller family film directed by Jiří Barta and written by Edgar Dutka and Barta which depicts a community of toys and other objects in an attic who come to life when no human is around. It is an international co-production of Czech, Japanese and Slovak companies. The film was released first in the Czech Republic on 5 March 2009 and has been shown subtitled at film festivals internationally.\n",
      "Margin: .. The Country Bears and The Wild are not related to each other. The Country Bears was released first in 2004...\n",
      "Answer: The Country Bears and The Wild were not related to each other. The Country B\n",
      "Reward: 0.5443257689476013\n",
      "Query: Which animated film was released first, The Country Bears or The Wild?\n",
      "Segment: An American dub – adapted, produced and directed by Vivian Schilling and performed by actors including Forest Whitaker, Joan Cusack, Cary Elwes and Schilling herself – has been recorded, which the film was first shown with on 3 March 2012 at the New York International Children's Film Festival and was released nationally on 24 August 2012 by Hannover House.\n",
      "Margin: \n",
      "The query \"Which animated film was released first, The Country Bears or The Wild?\" does not provide enough context to extract the relevant information.\n",
      "Reward: 0.6370000243186951\n",
      "ppo epoch: 0\n",
      "outputs.loss tensor(1.1527e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.3055e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.0130e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.1319e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(8.6090e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.2232e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 1\n",
      "outputs.loss tensor(1.1405e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.0824e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.8073e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.0706e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(8.3898e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.2363e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 2\n",
      "outputs.loss tensor(1.1083e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.8331e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.5838e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.0467e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(8.0605e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.1254e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 3\n",
      "outputs.loss tensor(1.1169e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.6760e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.4339e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.0706e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.5796e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.1135e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 4\n",
      "outputs.loss tensor(1.1131e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.6441e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.3569e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.0286e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.3667e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(2.0742e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Episode 5 Summary:\n",
      "  Average reward: 0.5305\n",
      "  No improvement in average reward for 2 episode(s).\n",
      "Training finished.\n",
      "\n",
      "--- Episode 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 7/7 [00:07<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: USS \"New Jersey\" (BB-62) (\"Big J\" or \"Black Dragon\") is an \"Iowa\"-class battleship , and was the second ship of the United States Navy to be named after the US state of New Jersey. \"New Jersey\" earned more battle stars for combat actions than the other three completed \"Iowa\"-class battleships, and was the only US battleship providing gunfire support during the Vietnam War. Operation Market Time was the United States Navy and South Vietnam’s successful effort begun in 1965 to stop the flow of troops, war material, and supplies by sea, coast, and rivers, from North Vietnam into parts of South Vietnam during the Vietnam War. Also participating in Operation Market Time were United States Coast Guard Squadron One and Squadron Three. The Coast Guard provided heavily armed 82 ft patrol boats and large cutters that included 5\" cannons used in battle and gunfire support. USS \"Trippe\" (FF-1075) was a \"Knox\"-class frigate of the US Navy, built at Westwego, Louisiana, was commissioned in mid-September 1970.\n",
      "Margin: \n",
      "Answer: The United States Navy and South Vietnam were both involved in the Vietnam War, and were both battleships.\n",
      "\n",
      "The United States Navy was commissioned in mid-September 1970.\n",
      "\n",
      "The United States\n",
      "Reward: 0.4329642951488495\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: In July 1971, following shakedown training in the Caribbean area and a surveillance mission off Haiti, she entered the Boston Naval Shipyard for overhaul and installation of the Basic Point Defense Missile System, which featured short-range \"Sea Sparrow\" guided missiles in an eight-round launcher on her afterdeck. \"Trippe\" was the Navy's first destroyer-type ship to receive this later-widespread contribution to shipboard protection against air and missile attack. The first months of 1972 were spent testing her new weapons and participating in exercises. In June the ship passed through the Panama Canal en route to Southeast Asian waters, where she provided Vietnam War aircraft carrier escort and naval gunfire support services during July and August. \"Trippe\" then went to the Indian Ocean and Persian Gulf areas, visiting many ports in a region that would see increasing U.S. Navy activity in the coming decades. She returned to the U.S. East Coast in December 1972, after a deployment that had taken her completely around the World. Rear Adm. J. Edward Snyder, USN (Ret.)\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Rear Adm. J. Edward Snyder, USN (Ret.).\n",
      "\n",
      "The\n",
      "Reward: 0.6207733750343323\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: (October 23, 1924 – November 4, 2007) was notable as the captain of the battleship USS \"New Jersey\" during that ship's deployment to the Vietnam War in 1968. Considered by those serving on the \"New Jersey\" to be a \"sailor's captain,\" Captain Snyder was able to motivate his men through his more relaxed shipboard policies. The United States naval gunfire support debate is an ongoing debate among the United States Navy, Marine Corps, Congress, and independent groups like the \"United States Naval Gunfire Support Association\" over what role naval gunfire support and naval surface fire support (NSFS) should play within the navy and how such a role can best be provided. At the heart of the issue is the role that naval gunfire support—the use of naval artillery to provide fire support for amphibious assault and other troops operating within their range—should play in the U.S. Navy of the 21st century.\n",
      "Margin: \n",
      "Answer: The United States Navy and Marine Corps were both involved in the Vietnam War and have been in the Navy and Marine Corps since 1964.\n",
      "Reward: 0.404208779335022\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: Naval gunfire support (NGFS) (also known as shore bombardment) is the use of naval artillery to provide fire support for amphibious assault and other troops operating within their range. NGFS is one of a number of disciplines encompassed by the term \"Naval Fires\". Modern naval gunfire support is one of the three main components of amphibious warfare assault operations support, along with aircraft and ship-launched land-attack missiles. Shipborne guns have been used against shore defences since the early days of naval warfare. Tonkin Gulf Yacht Club was a tongue-in-cheek nickname for the United States Seventh Fleet during the Vietnam War. All through the war in Vietnam, the Seventh Fleet engaged in combat operations against enemy forces through attack carrier air strikes, naval gunfire support, amphibious operations, patrol and reconnaissance operations and mine warfare. Starting in the 1890s, the Italian \"Regia Marina\" (Royal Navy) began building a series of modern battleships. Early designs were marked by their small size, light armor, and high speed compared to contemporary foreign counterparts.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Vietnam War was fought by the Italian \"Regia Marina\" (Royal Navy) in 1890...\n",
      "\n",
      "The relevant information to answer the question is:\n",
      "Reward: 0.4145783483982086\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: The first pre-dreadnought battleship design, the \"Ammiraglio di Saint Bon\" class , was constrained by budgetary limits imposed by the legislature. Two ships were ordered by the class's namesake, Admiral Simone de Pacoret Saint Bon, though the design was also influenced by Benedetto Brin, who replaced di Saint Bon as naval minister after his death. Brin designed the next pair of battleships, the \"Regina Margherita\" class . These ships were larger than the preceding class, and were intended to challenge the Austro-Hungarian \"Habsburg\"-class battleship s then under construction. Brin himself died during the construction process. Vittorio Cuniberti designed the next class of small pre-dreadnoughts, the \"Regina Elena\" class , which were the fastest battleships in the world at the time of their completion.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Vietnam War was fought between the Republic of Vietnam and the Kingdom of France.\n",
      "Reward: 0.4405714273452759\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: These ships all served in the Italo-Turkish War of 1911–12, where they were primarily used to provide naval gunfire support for the Italian ground troops, as the Ottoman Navy largely confined itself to port. Kirov (Russian: Киров ; ] ) was a Project 26 \"Kirov\"-class cruiser of the Soviet Navy that served during the Winter War, World War II and into the Cold War. She attempted to bombard Finnish coast defense guns during action in the Winter War, but was driven off by a number of near misses that damaged her. She led the Evacuation of Tallinn at the end of August 1941, before being blockaded in Leningrad where she could only provide gunfire support during the Siege of Leningrad. She bombarded Finnish positions during the Vyborg–Petrozavodsk Offensive in mid-1944, but played no further part in the war. \"Kirov\" was reclassified as a training cruiser on 2 August 1961 and sold for scrap on 22 February 1974.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Kirov.\n",
      "Reward: 0.5930714011192322\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: Jean Bart was the second ship of the \"Courbet\"-class battleship s, the first dreadnoughts built for the French Navy. She was completed before World War I as part of the 1910 naval building programme. She spent the war in the Mediterranean and helped to sink the Austro-Hungarian protected cruiser \"Zenta\" on 16 August 1914. She spent most of the rest of 1914 providing gunfire support for the Montenegrin Army until she was torpedoed by the submarine \"U-12\" on 21 December. Even with three compartments flooded, she was able to steam to Malta on her own for repairs that required three and a half months. Upon her return she spent the remainder of the war participating in the Otranto Barrage, in the Adriatic.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The captain of the only battleship to provide gunfire support during the Vietnam War was Jean Bart.\n",
      "Reward: 0.5954175591468811\n",
      "ppo epoch: 0\n",
      "outputs.loss tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.5757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.6264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.8905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 1\n",
      "outputs.loss tensor(0.4327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.3813, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 2\n",
      "outputs.loss tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.3039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.7615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.4410, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 3\n",
      "outputs.loss tensor(0.2088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.4411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 4\n",
      "outputs.loss tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.4242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.2675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Episode 1 Summary:\n",
      "  Average reward: 0.5002\n",
      "  New best average reward: 0.5002 (Improvement)\n",
      "\n",
      "--- Episode 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 7/7 [00:07<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: USS \"New Jersey\" (BB-62) (\"Big J\" or \"Black Dragon\") is an \"Iowa\"-class battleship , and was the second ship of the United States Navy to be named after the US state of New Jersey. \"New Jersey\" earned more battle stars for combat actions than the other three completed \"Iowa\"-class battleships, and was the only US battleship providing gunfire support during the Vietnam War. Operation Market Time was the United States Navy and South Vietnam’s successful effort begun in 1965 to stop the flow of troops, war material, and supplies by sea, coast, and rivers, from North Vietnam into parts of South Vietnam during the Vietnam War. Also participating in Operation Market Time were United States Coast Guard Squadron One and Squadron Three. The Coast Guard provided heavily armed 82 ft patrol boats and large cutters that included 5\" cannons used in battle and gunfire support. USS \"Trippe\" (FF-1075) was a \"Knox\"-class frigate of the US Navy, built at Westwego, Louisiana, was commissioned in mid-September 1970.\n",
      "Margin: \n",
      "Answer: The United States Navy and South Vietnam were both involved in the Vietnam War, and were both battleships.\n",
      "Reward: 0.4079962372779846\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: In July 1971, following shakedown training in the Caribbean area and a surveillance mission off Haiti, she entered the Boston Naval Shipyard for overhaul and installation of the Basic Point Defense Missile System, which featured short-range \"Sea Sparrow\" guided missiles in an eight-round launcher on her afterdeck. \"Trippe\" was the Navy's first destroyer-type ship to receive this later-widespread contribution to shipboard protection against air and missile attack. The first months of 1972 were spent testing her new weapons and participating in exercises. In June the ship passed through the Panama Canal en route to Southeast Asian waters, where she provided Vietnam War aircraft carrier escort and naval gunfire support services during July and August. \"Trippe\" then went to the Indian Ocean and Persian Gulf areas, visiting many ports in a region that would see increasing U.S. Navy activity in the coming decades. She returned to the U.S. East Coast in December 1972, after a deployment that had taken her completely around the World. Rear Adm. J. Edward Snyder, USN (Ret.)\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Rear Admiler.\n",
      "Reward: 0.5954175591468811\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: (October 23, 1924 – November 4, 2007) was notable as the captain of the battleship USS \"New Jersey\" during that ship's deployment to the Vietnam War in 1968. Considered by those serving on the \"New Jersey\" to be a \"sailor's captain,\" Captain Snyder was able to motivate his men through his more relaxed shipboard policies. The United States naval gunfire support debate is an ongoing debate among the United States Navy, Marine Corps, Congress, and independent groups like the \"United States Naval Gunfire Support Association\" over what role naval gunfire support and naval surface fire support (NSFS) should play within the navy and how such a role can best be provided. At the heart of the issue is the role that naval gunfire support—the use of naval artillery to provide fire support for amphibious assault and other troops operating within their range—should play in the U.S. Navy of the 21st century.\n",
      "Margin: \n",
      "Answer: The United States Navy and Marine Corps were both involved in the Vietnam War and have been in the Navy and Marine Corps since 1964.\n",
      "Reward: 0.404208779335022\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: Naval gunfire support (NGFS) (also known as shore bombardment) is the use of naval artillery to provide fire support for amphibious assault and other troops operating within their range. NGFS is one of a number of disciplines encompassed by the term \"Naval Fires\". Modern naval gunfire support is one of the three main components of amphibious warfare assault operations support, along with aircraft and ship-launched land-attack missiles. Shipborne guns have been used against shore defences since the early days of naval warfare. Tonkin Gulf Yacht Club was a tongue-in-cheek nickname for the United States Seventh Fleet during the Vietnam War. All through the war in Vietnam, the Seventh Fleet engaged in combat operations against enemy forces through attack carrier air strikes, naval gunfire support, amphibious operations, patrol and reconnaissance operations and mine warfare. Starting in the 1890s, the Italian \"Regia Marina\" (Royal Navy) began building a series of modern battleships. Early designs were marked by their small size, light armor, and high speed compared to contemporary foreign counterparts.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Jean Bart.\n",
      "Reward: 0.5954175591468811\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: The first pre-dreadnought battleship design, the \"Ammiraglio di Saint Bon\" class , was constrained by budgetary limits imposed by the legislature. Two ships were ordered by the class's namesake, Admiral Simone de Pacoret Saint Bon, though the design was also influenced by Benedetto Brin, who replaced di Saint Bon as naval minister after his death. Brin designed the next pair of battleships, the \"Regina Margherita\" class . These ships were larger than the preceding class, and were intended to challenge the Austro-Hungarian \"Habsburg\"-class battleship s then under construction. Brin himself died during the construction process. Vittorio Cuniberti designed the next class of small pre-dreadnoughts, the \"Regina Elena\" class , which were the fastest battleships in the world at the time of their completion.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Vietnam War was fought between the Republic of Vietnam and the Kingdom of France.\n",
      "Reward: 0.4405714273452759\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: These ships all served in the Italo-Turkish War of 1911–12, where they were primarily used to provide naval gunfire support for the Italian ground troops, as the Ottoman Navy largely confined itself to port. Kirov (Russian: Киров ; ] ) was a Project 26 \"Kirov\"-class cruiser of the Soviet Navy that served during the Winter War, World War II and into the Cold War. She attempted to bombard Finnish coast defense guns during action in the Winter War, but was driven off by a number of near misses that damaged her. She led the Evacuation of Tallinn at the end of August 1941, before being blockaded in Leningrad where she could only provide gunfire support during the Siege of Leningrad. She bombarded Finnish positions during the Vyborg–Petrozavodsk Offensive in mid-1944, but played no further part in the war. \"Kirov\" was reclassified as a training cruiser on 2 August 1961 and sold for scrap on 22 February 1974.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Kirov.\n",
      "Reward: 0.5930714011192322\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: Jean Bart was the second ship of the \"Courbet\"-class battleship s, the first dreadnoughts built for the French Navy. She was completed before World War I as part of the 1910 naval building programme. She spent the war in the Mediterranean and helped to sink the Austro-Hungarian protected cruiser \"Zenta\" on 16 August 1914. She spent most of the rest of 1914 providing gunfire support for the Montenegrin Army until she was torpedoed by the submarine \"U-12\" on 21 December. Even with three compartments flooded, she was able to steam to Malta on her own for repairs that required three and a half months. Upon her return she spent the remainder of the war participating in the Otranto Barrage, in the Adriatic.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Jean Bart.\n",
      "Reward: 0.5954175591468811\n",
      "ppo epoch: 0\n",
      "outputs.loss tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 1\n",
      "outputs.loss tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(8.4304e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 2\n",
      "outputs.loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.8640e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 3\n",
      "outputs.loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.8296e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(8.9170e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 4\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.7602e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.6136e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.3988e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(6.9545e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Episode 2 Summary:\n",
      "  Average reward: 0.5189\n",
      "  New best average reward: 0.5189 (Improvement)\n",
      "\n",
      "--- Episode 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 7/7 [00:07<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: USS \"New Jersey\" (BB-62) (\"Big J\" or \"Black Dragon\") is an \"Iowa\"-class battleship , and was the second ship of the United States Navy to be named after the US state of New Jersey. \"New Jersey\" earned more battle stars for combat actions than the other three completed \"Iowa\"-class battleships, and was the only US battleship providing gunfire support during the Vietnam War. Operation Market Time was the United States Navy and South Vietnam’s successful effort begun in 1965 to stop the flow of troops, war material, and supplies by sea, coast, and rivers, from North Vietnam into parts of South Vietnam during the Vietnam War. Also participating in Operation Market Time were United States Coast Guard Squadron One and Squadron Three. The Coast Guard provided heavily armed 82 ft patrol boats and large cutters that included 5\" cannons used in battle and gunfire support. USS \"Trippe\" (FF-1075) was a \"Knox\"-class frigate of the US Navy, built at Westwego, Louisiana, was commissioned in mid-September 1970.\n",
      "Margin: \n",
      "Answer: The United States Navy and South Vietnam were both involved in the Vietnam War, and were both battleships.\n",
      "Reward: 0.4079962372779846\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: In July 1971, following shakedown training in the Caribbean area and a surveillance mission off Haiti, she entered the Boston Naval Shipyard for overhaul and installation of the Basic Point Defense Missile System, which featured short-range \"Sea Sparrow\" guided missiles in an eight-round launcher on her afterdeck. \"Trippe\" was the Navy's first destroyer-type ship to receive this later-widespread contribution to shipboard protection against air and missile attack. The first months of 1972 were spent testing her new weapons and participating in exercises. In June the ship passed through the Panama Canal en route to Southeast Asian waters, where she provided Vietnam War aircraft carrier escort and naval gunfire support services during July and August. \"Trippe\" then went to the Indian Ocean and Persian Gulf areas, visiting many ports in a region that would see increasing U.S. Navy activity in the coming decades. She returned to the U.S. East Coast in December 1972, after a deployment that had taken her completely around the World. Rear Adm. J. Edward Snyder, USN (Ret.)\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Rear Admiler.\n",
      "Reward: 0.5954175591468811\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: (October 23, 1924 – November 4, 2007) was notable as the captain of the battleship USS \"New Jersey\" during that ship's deployment to the Vietnam War in 1968. Considered by those serving on the \"New Jersey\" to be a \"sailor's captain,\" Captain Snyder was able to motivate his men through his more relaxed shipboard policies. The United States naval gunfire support debate is an ongoing debate among the United States Navy, Marine Corps, Congress, and independent groups like the \"United States Naval Gunfire Support Association\" over what role naval gunfire support and naval surface fire support (NSFS) should play within the navy and how such a role can best be provided. At the heart of the issue is the role that naval gunfire support—the use of naval artillery to provide fire support for amphibious assault and other troops operating within their range—should play in the U.S. Navy of the 21st century.\n",
      "Margin: \n",
      "Answer: The United States Navy and Marine Corps were both involved in the Vietnam War and have been in the Navy and Marine Corps since 1964.\n",
      "Reward: 0.404208779335022\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: Naval gunfire support (NGFS) (also known as shore bombardment) is the use of naval artillery to provide fire support for amphibious assault and other troops operating within their range. NGFS is one of a number of disciplines encompassed by the term \"Naval Fires\". Modern naval gunfire support is one of the three main components of amphibious warfare assault operations support, along with aircraft and ship-launched land-attack missiles. Shipborne guns have been used against shore defences since the early days of naval warfare. Tonkin Gulf Yacht Club was a tongue-in-cheek nickname for the United States Seventh Fleet during the Vietnam War. All through the war in Vietnam, the Seventh Fleet engaged in combat operations against enemy forces through attack carrier air strikes, naval gunfire support, amphibious operations, patrol and reconnaissance operations and mine warfare. Starting in the 1890s, the Italian \"Regia Marina\" (Royal Navy) began building a series of modern battleships. Early designs were marked by their small size, light armor, and high speed compared to contemporary foreign counterparts.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Jean Bart.\n",
      "Reward: 0.5954175591468811\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: The first pre-dreadnought battleship design, the \"Ammiraglio di Saint Bon\" class , was constrained by budgetary limits imposed by the legislature. Two ships were ordered by the class's namesake, Admiral Simone de Pacoret Saint Bon, though the design was also influenced by Benedetto Brin, who replaced di Saint Bon as naval minister after his death. Brin designed the next pair of battleships, the \"Regina Margherita\" class . These ships were larger than the preceding class, and were intended to challenge the Austro-Hungarian \"Habsburg\"-class battleship s then under construction. Brin himself died during the construction process. Vittorio Cuniberti designed the next class of small pre-dreadnoughts, the \"Regina Elena\" class , which were the fastest battleships in the world at the time of their completion.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Vietnam War was fought between the Republic of Vietnam and the Kingdom of France.\n",
      "Reward: 0.4405714273452759\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: These ships all served in the Italo-Turkish War of 1911–12, where they were primarily used to provide naval gunfire support for the Italian ground troops, as the Ottoman Navy largely confined itself to port. Kirov (Russian: Киров ; ] ) was a Project 26 \"Kirov\"-class cruiser of the Soviet Navy that served during the Winter War, World War II and into the Cold War. She attempted to bombard Finnish coast defense guns during action in the Winter War, but was driven off by a number of near misses that damaged her. She led the Evacuation of Tallinn at the end of August 1941, before being blockaded in Leningrad where she could only provide gunfire support during the Siege of Leningrad. She bombarded Finnish positions during the Vyborg–Petrozavodsk Offensive in mid-1944, but played no further part in the war. \"Kirov\" was reclassified as a training cruiser on 2 August 1961 and sold for scrap on 22 February 1974.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Kirov.\n",
      "Reward: 0.5930714011192322\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: Jean Bart was the second ship of the \"Courbet\"-class battleship s, the first dreadnoughts built for the French Navy. She was completed before World War I as part of the 1910 naval building programme. She spent the war in the Mediterranean and helped to sink the Austro-Hungarian protected cruiser \"Zenta\" on 16 August 1914. She spent most of the rest of 1914 providing gunfire support for the Montenegrin Army until she was torpedoed by the submarine \"U-12\" on 21 December. Even with three compartments flooded, she was able to steam to Malta on her own for repairs that required three and a half months. Upon her return she spent the remainder of the war participating in the Otranto Barrage, in the Adriatic.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Jean Bart.\n",
      "Reward: 0.5954175591468811\n",
      "ppo epoch: 0\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(6.5463e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(8.5774e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.2673e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(6.3975e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 1\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.6770e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(8.2332e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.2721e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.9625e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 2\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.2879e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.7869e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.2152e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.8727e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 3\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.2313e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.6789e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.2360e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.6291e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 4\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.6796e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.6652e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.2282e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.6357e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Episode 3 Summary:\n",
      "  Average reward: 0.5189\n",
      "  No improvement in average reward for 1 episode(s).\n",
      "\n",
      "--- Episode 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 7/7 [00:07<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: USS \"New Jersey\" (BB-62) (\"Big J\" or \"Black Dragon\") is an \"Iowa\"-class battleship , and was the second ship of the United States Navy to be named after the US state of New Jersey. \"New Jersey\" earned more battle stars for combat actions than the other three completed \"Iowa\"-class battleships, and was the only US battleship providing gunfire support during the Vietnam War. Operation Market Time was the United States Navy and South Vietnam’s successful effort begun in 1965 to stop the flow of troops, war material, and supplies by sea, coast, and rivers, from North Vietnam into parts of South Vietnam during the Vietnam War. Also participating in Operation Market Time were United States Coast Guard Squadron One and Squadron Three. The Coast Guard provided heavily armed 82 ft patrol boats and large cutters that included 5\" cannons used in battle and gunfire support. USS \"Trippe\" (FF-1075) was a \"Knox\"-class frigate of the US Navy, built at Westwego, Louisiana, was commissioned in mid-September 1970.\n",
      "Margin: \n",
      "Answer: The United States Navy and South Vietnam were both involved in the Vietnam War, and were both battleships.\n",
      "Reward: 0.4079962372779846\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: In July 1971, following shakedown training in the Caribbean area and a surveillance mission off Haiti, she entered the Boston Naval Shipyard for overhaul and installation of the Basic Point Defense Missile System, which featured short-range \"Sea Sparrow\" guided missiles in an eight-round launcher on her afterdeck. \"Trippe\" was the Navy's first destroyer-type ship to receive this later-widespread contribution to shipboard protection against air and missile attack. The first months of 1972 were spent testing her new weapons and participating in exercises. In June the ship passed through the Panama Canal en route to Southeast Asian waters, where she provided Vietnam War aircraft carrier escort and naval gunfire support services during July and August. \"Trippe\" then went to the Indian Ocean and Persian Gulf areas, visiting many ports in a region that would see increasing U.S. Navy activity in the coming decades. She returned to the U.S. East Coast in December 1972, after a deployment that had taken her completely around the World. Rear Adm. J. Edward Snyder, USN (Ret.)\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Rear Admiler.\n",
      "Reward: 0.5954175591468811\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: (October 23, 1924 – November 4, 2007) was notable as the captain of the battleship USS \"New Jersey\" during that ship's deployment to the Vietnam War in 1968. Considered by those serving on the \"New Jersey\" to be a \"sailor's captain,\" Captain Snyder was able to motivate his men through his more relaxed shipboard policies. The United States naval gunfire support debate is an ongoing debate among the United States Navy, Marine Corps, Congress, and independent groups like the \"United States Naval Gunfire Support Association\" over what role naval gunfire support and naval surface fire support (NSFS) should play within the navy and how such a role can best be provided. At the heart of the issue is the role that naval gunfire support—the use of naval artillery to provide fire support for amphibious assault and other troops operating within their range—should play in the U.S. Navy of the 21st century.\n",
      "Margin: \n",
      "Answer: The United States Navy and Marine Corps were both involved in the Vietnam War and have been in the Navy and Marine Corps since 1964.\n",
      "Reward: 0.404208779335022\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: Naval gunfire support (NGFS) (also known as shore bombardment) is the use of naval artillery to provide fire support for amphibious assault and other troops operating within their range. NGFS is one of a number of disciplines encompassed by the term \"Naval Fires\". Modern naval gunfire support is one of the three main components of amphibious warfare assault operations support, along with aircraft and ship-launched land-attack missiles. Shipborne guns have been used against shore defences since the early days of naval warfare. Tonkin Gulf Yacht Club was a tongue-in-cheek nickname for the United States Seventh Fleet during the Vietnam War. All through the war in Vietnam, the Seventh Fleet engaged in combat operations against enemy forces through attack carrier air strikes, naval gunfire support, amphibious operations, patrol and reconnaissance operations and mine warfare. Starting in the 1890s, the Italian \"Regia Marina\" (Royal Navy) began building a series of modern battleships. Early designs were marked by their small size, light armor, and high speed compared to contemporary foreign counterparts.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Jean Bart.\n",
      "Reward: 0.5954175591468811\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: The first pre-dreadnought battleship design, the \"Ammiraglio di Saint Bon\" class , was constrained by budgetary limits imposed by the legislature. Two ships were ordered by the class's namesake, Admiral Simone de Pacoret Saint Bon, though the design was also influenced by Benedetto Brin, who replaced di Saint Bon as naval minister after his death. Brin designed the next pair of battleships, the \"Regina Margherita\" class . These ships were larger than the preceding class, and were intended to challenge the Austro-Hungarian \"Habsburg\"-class battleship s then under construction. Brin himself died during the construction process. Vittorio Cuniberti designed the next class of small pre-dreadnoughts, the \"Regina Elena\" class , which were the fastest battleships in the world at the time of their completion.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Vietnam War was fought between the Republic of Vietnam and the Kingdom of France.\n",
      "Reward: 0.4405714273452759\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: These ships all served in the Italo-Turkish War of 1911–12, where they were primarily used to provide naval gunfire support for the Italian ground troops, as the Ottoman Navy largely confined itself to port. Kirov (Russian: Киров ; ] ) was a Project 26 \"Kirov\"-class cruiser of the Soviet Navy that served during the Winter War, World War II and into the Cold War. She attempted to bombard Finnish coast defense guns during action in the Winter War, but was driven off by a number of near misses that damaged her. She led the Evacuation of Tallinn at the end of August 1941, before being blockaded in Leningrad where she could only provide gunfire support during the Siege of Leningrad. She bombarded Finnish positions during the Vyborg–Petrozavodsk Offensive in mid-1944, but played no further part in the war. \"Kirov\" was reclassified as a training cruiser on 2 August 1961 and sold for scrap on 22 February 1974.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Kirov.\n",
      "Reward: 0.5930714011192322\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: Jean Bart was the second ship of the \"Courbet\"-class battleship s, the first dreadnoughts built for the French Navy. She was completed before World War I as part of the 1910 naval building programme. She spent the war in the Mediterranean and helped to sink the Austro-Hungarian protected cruiser \"Zenta\" on 16 August 1914. She spent most of the rest of 1914 providing gunfire support for the Montenegrin Army until she was torpedoed by the submarine \"U-12\" on 21 December. Even with three compartments flooded, she was able to steam to Malta on her own for repairs that required three and a half months. Upon her return she spent the remainder of the war participating in the Otranto Barrage, in the Adriatic.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Jean Bart.\n",
      "Reward: 0.5954175591468811\n",
      "ppo epoch: 0\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.6402e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.4784e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.2013e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.5856e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 1\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.3638e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.4675e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.1774e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.4776e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 2\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.4638e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.4135e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.1607e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.3521e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 3\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.0689e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.6313e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.1033e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.0868e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.2571e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 4\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.1265e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.4912e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.0844e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.1383e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.1383e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Episode 4 Summary:\n",
      "  Average reward: 0.5189\n",
      "  No improvement in average reward for 2 episode(s).\n",
      "\n",
      "--- Episode 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating margins: 100%|██████████| 7/7 [00:07<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: USS \"New Jersey\" (BB-62) (\"Big J\" or \"Black Dragon\") is an \"Iowa\"-class battleship , and was the second ship of the United States Navy to be named after the US state of New Jersey. \"New Jersey\" earned more battle stars for combat actions than the other three completed \"Iowa\"-class battleships, and was the only US battleship providing gunfire support during the Vietnam War. Operation Market Time was the United States Navy and South Vietnam’s successful effort begun in 1965 to stop the flow of troops, war material, and supplies by sea, coast, and rivers, from North Vietnam into parts of South Vietnam during the Vietnam War. Also participating in Operation Market Time were United States Coast Guard Squadron One and Squadron Three. The Coast Guard provided heavily armed 82 ft patrol boats and large cutters that included 5\" cannons used in battle and gunfire support. USS \"Trippe\" (FF-1075) was a \"Knox\"-class frigate of the US Navy, built at Westwego, Louisiana, was commissioned in mid-September 1970.\n",
      "Margin: \n",
      "Answer: The United States Navy and South Vietnam were both involved in the Vietnam War, and were both battleships.\n",
      "Reward: 0.4079962372779846\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: In July 1971, following shakedown training in the Caribbean area and a surveillance mission off Haiti, she entered the Boston Naval Shipyard for overhaul and installation of the Basic Point Defense Missile System, which featured short-range \"Sea Sparrow\" guided missiles in an eight-round launcher on her afterdeck. \"Trippe\" was the Navy's first destroyer-type ship to receive this later-widespread contribution to shipboard protection against air and missile attack. The first months of 1972 were spent testing her new weapons and participating in exercises. In June the ship passed through the Panama Canal en route to Southeast Asian waters, where she provided Vietnam War aircraft carrier escort and naval gunfire support services during July and August. \"Trippe\" then went to the Indian Ocean and Persian Gulf areas, visiting many ports in a region that would see increasing U.S. Navy activity in the coming decades. She returned to the U.S. East Coast in December 1972, after a deployment that had taken her completely around the World. Rear Adm. J. Edward Snyder, USN (Ret.)\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Rear Admiler.\n",
      "Reward: 0.5954175591468811\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: (October 23, 1924 – November 4, 2007) was notable as the captain of the battleship USS \"New Jersey\" during that ship's deployment to the Vietnam War in 1968. Considered by those serving on the \"New Jersey\" to be a \"sailor's captain,\" Captain Snyder was able to motivate his men through his more relaxed shipboard policies. The United States naval gunfire support debate is an ongoing debate among the United States Navy, Marine Corps, Congress, and independent groups like the \"United States Naval Gunfire Support Association\" over what role naval gunfire support and naval surface fire support (NSFS) should play within the navy and how such a role can best be provided. At the heart of the issue is the role that naval gunfire support—the use of naval artillery to provide fire support for amphibious assault and other troops operating within their range—should play in the U.S. Navy of the 21st century.\n",
      "Margin: \n",
      "Answer: The United States Navy and Marine Corps were both involved in the Vietnam War and have been in the Navy and Marine Corps since 1964.\n",
      "Reward: 0.404208779335022\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: Naval gunfire support (NGFS) (also known as shore bombardment) is the use of naval artillery to provide fire support for amphibious assault and other troops operating within their range. NGFS is one of a number of disciplines encompassed by the term \"Naval Fires\". Modern naval gunfire support is one of the three main components of amphibious warfare assault operations support, along with aircraft and ship-launched land-attack missiles. Shipborne guns have been used against shore defences since the early days of naval warfare. Tonkin Gulf Yacht Club was a tongue-in-cheek nickname for the United States Seventh Fleet during the Vietnam War. All through the war in Vietnam, the Seventh Fleet engaged in combat operations against enemy forces through attack carrier air strikes, naval gunfire support, amphibious operations, patrol and reconnaissance operations and mine warfare. Starting in the 1890s, the Italian \"Regia Marina\" (Royal Navy) began building a series of modern battleships. Early designs were marked by their small size, light armor, and high speed compared to contemporary foreign counterparts.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Jean Bart.\n",
      "Reward: 0.5954175591468811\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: The first pre-dreadnought battleship design, the \"Ammiraglio di Saint Bon\" class , was constrained by budgetary limits imposed by the legislature. Two ships were ordered by the class's namesake, Admiral Simone de Pacoret Saint Bon, though the design was also influenced by Benedetto Brin, who replaced di Saint Bon as naval minister after his death. Brin designed the next pair of battleships, the \"Regina Margherita\" class . These ships were larger than the preceding class, and were intended to challenge the Austro-Hungarian \"Habsburg\"-class battleship s then under construction. Brin himself died during the construction process. Vittorio Cuniberti designed the next class of small pre-dreadnoughts, the \"Regina Elena\" class , which were the fastest battleships in the world at the time of their completion.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Vietnam War was fought between the Republic of Vietnam and the Kingdom of France.\n",
      "Reward: 0.4405714273452759\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: These ships all served in the Italo-Turkish War of 1911–12, where they were primarily used to provide naval gunfire support for the Italian ground troops, as the Ottoman Navy largely confined itself to port. Kirov (Russian: Киров ; ] ) was a Project 26 \"Kirov\"-class cruiser of the Soviet Navy that served during the Winter War, World War II and into the Cold War. She attempted to bombard Finnish coast defense guns during action in the Winter War, but was driven off by a number of near misses that damaged her. She led the Evacuation of Tallinn at the end of August 1941, before being blockaded in Leningrad where she could only provide gunfire support during the Siege of Leningrad. She bombarded Finnish positions during the Vyborg–Petrozavodsk Offensive in mid-1944, but played no further part in the war. \"Kirov\" was reclassified as a training cruiser on 2 August 1961 and sold for scrap on 22 February 1974.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Kirov.\n",
      "Reward: 0.5930714011192322\n",
      "Query: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?\n",
      "Segment: Jean Bart was the second ship of the \"Courbet\"-class battleship s, the first dreadnoughts built for the French Navy. She was completed before World War I as part of the 1910 naval building programme. She spent the war in the Mediterranean and helped to sink the Austro-Hungarian protected cruiser \"Zenta\" on 16 August 1914. She spent most of the rest of 1914 providing gunfire support for the Montenegrin Army until she was torpedoed by the submarine \"U-12\" on 21 December. Even with three compartments flooded, she was able to steam to Malta on her own for repairs that required three and a half months. Upon her return she spent the remainder of the war participating in the Otranto Barrage, in the Adriatic.\n",
      "Margin: \n",
      "Answer: The relevant information to answer the question is: \"The Captain of the only battleship to provide gunfire support during the Vietnam War was Jean Bart.\n",
      "Reward: 0.5954175591468811\n",
      "ppo epoch: 0\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.1241e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.2224e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(6.9862e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.0742e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.1173e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 1\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.1986e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.3373e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(6.9883e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.0821e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.1765e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 2\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.1092e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.4253e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(6.9508e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.1318e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(4.9896e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 3\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.9411e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.5944e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(7.0840e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.0947e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.1109e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ppo epoch: 4\n",
      "outputs.loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(3.9216e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(9.6277e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(6.9147e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(1.1110e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "outputs.loss tensor(5.0924e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Episode 5 Summary:\n",
      "  Average reward: 0.5189\n",
      "  No improvement in average reward for 3 episode(s).\n",
      "No improvement for 3 episodes. Stopping early.\n",
      "Training finished.\n",
      "avg reward for threshold\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"h2oai/h2o-danube3-500m-base\" \n",
    "model_id_rl = 'h2oai/h2o-danube3-500m-base'\n",
    "dir = '/kaggle/working/'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",attn_implementation='eager', torch_dtype='bfloat16')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map=\"auto\",attn_implementation='eager', torch_dtype='bfloat16')\n",
    "\n",
    "rl_margin_generator = RLMargin_Generator(\n",
    "            model_id=model,\n",
    "            tokenizer=tokenizer,\n",
    "            rl_config=RLConfig(),\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available else \"cpu\",\n",
    "        )\n",
    "count=0\n",
    "for batch in train_dataloader:\n",
    "    for segment, query, supporting_facts in zip(batch['segment'], batch['query'], batch['supporting_facts']):\n",
    "        count+=1\n",
    "        if count > 2:\n",
    "            break\n",
    "        rl_margin_generator.train(\n",
    "            segments = segment,\n",
    "            query = query,\n",
    "            extractive_summary_prompt = template_extractive_summary,\n",
    "            num_episodes = 5,\n",
    "            max_new_tokens = 50,\n",
    "            supporting_facts = supporting_facts,\n",
    "            min_tokens_segment=256,\n",
    "        )\n",
    "print(\"avg reward for threshold\")\n",
    "rl_margin_generator.compute_avg_reward()\n",
    "# for batch in val_dataloader:\n",
    "#     for segment, query, supporting_facts in zip(batch['segment'], batch['query'], batch['supporting_facts']):\n",
    "#         rl_margin_generator.train(\n",
    "#             segments = segment,\n",
    "#             query = query,\n",
    "#             extractive_summary_prompt = template_extractive_summary,\n",
    "#             num_episodes = 1,\n",
    "#             max_new_tokens = 50,\n",
    "#             supporting_facts = supporting_facts,\n",
    "#             min_tokens_segment=256,\n",
    "#         )\n",
    "\n",
    "# for i in range(10):\n",
    "#     query = training_data[i]['query']\n",
    "#     context = training_data[i]['segment']\n",
    "#     supporting_facts = training_data[i]['supporting_facts']\n",
    "#     # print(supporting_facts)\n",
    "#     # print(type(supporting_facts))\n",
    "#     rl_margin_generator.train(\n",
    "#         segments = context,\n",
    "#         query = query,\n",
    "#         extractive_summary_prompt = template_extractive_summary,\n",
    "#         num_episodes = 1,\n",
    "#         max_new_tokens = 50,\n",
    "#         supporting_facts = supporting_facts,\n",
    "#         min_tokens_segment=256,\n",
    "#     )\n",
    "\n",
    "# rl_margin_generator.save_model_output(dir)\n",
    "\n",
    "# INCREASE THE EPOCHS PER MARGIN GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel\n",
    "\n",
    "# model = AutoModel.from_pretrained('h2oai/h2o-danube3-500m-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "706e317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "335b5047",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_final_answer = \"\"\"\n",
    "```\n",
    "{margins}\n",
    "{query}\n",
    "\"\"\".strip()\n",
    "\n",
    "# special_tokens = {\n",
    "#     \"{user_header}\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "#     \"{generation_header}\": \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "# }\n",
    "\n",
    "# for token, replacement in special_tokens.items():\n",
    "#     template_extractive_summary = template_extractive_summary.replace(token, replacement)\n",
    "#     template_system_message = template_system_message.replace(token, replacement)\n",
    "#     template_final_answer = template_final_answer.replace(token, replacement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e80690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"h2oai/h2o-danube3-500m-base\"  # or any compatible model identifier\n",
    "model_id_rl = 'h2oai/h2o-danube3-500m-base'\n",
    "dir = '/kaggle/working/'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",attn_implementation='eager', torch_dtype='bfloat16')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map=\"auto\",attn_implementation='eager', torch_dtype='bfloat16')\n",
    "\n",
    "rl_margin_generator = RLMargin_Generator(\n",
    "            model_id=model,\n",
    "            tokenizer = tokenizer,\n",
    "            rl_config=RLConfig(),\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available else \"cpu\",\n",
    "        )\n",
    "\n",
    "wim_inference = WIMInference(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "final_output = []\n",
    "count = 0\n",
    "for batch in val_dataloader:\n",
    "    for segment, query, supporting_facts, answer in zip(batch['segment'], batch['query'], batch['supporting_facts'], batch['answer']):\n",
    "        count += 1\n",
    "        if(count >100):\n",
    "            break\n",
    "        query = query\n",
    "        context = segment\n",
    "        # supporting_facts = test_data[i]['supporting_facts']\n",
    "        print('context:', context)\n",
    "        print('query:', query)\n",
    "        # chunk segment\n",
    "        # chunked prefill\n",
    "        # margins .append with certain threshold\n",
    "        # \n",
    "        margins = []\n",
    "        segments = chunk_text_to_segments(text = context, min_tokens_segment=1024, tokenizer=tokenizer)\n",
    "        for segment in segments:\n",
    "            margin = rl_margin_generator.generate_rl_margin(\n",
    "                segment = segment,\n",
    "                query = query,\n",
    "                extractive_summary = template_extractive_summary,\n",
    "                supporting_facts = supporting_facts,\n",
    "                max_new_tokens = 50,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7,\n",
    "                early_stopping=True,\n",
    "                remove_segment=False\n",
    "            )\n",
    "\n",
    "            print(f'margin: {margin}')\n",
    "            print(f'segment: {segment}')\n",
    "            margins.append(margin)\n",
    "\n",
    "        # rewards = compute_reward(margins, query, supporting_facts, device=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available else \"cpu\")\n",
    "        # print('rewards:', rewards)\n",
    "\n",
    "        formatted_final_answer = template_final_answer.format(\n",
    "            margins =margins, query = query\n",
    "        )\n",
    "\n",
    "        _, _, final_answer_prefill_outputs = wim_inference.prefill_text_with_kv_cache(\n",
    "            formatted_final_answer, wim_inference.wim_kv_cache\n",
    "        )\n",
    "\n",
    "        final_answer = wim_inference.generate_text_with_kv_cache(\n",
    "            max_new_tokens=20,\n",
    "            previous_logits=final_answer_prefill_outputs[\"logits\"],\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            early_stopping=True,\n",
    "            kv_cache=wim_inference.wim_kv_cache,\n",
    "        )\n",
    "\n",
    "        print('final_answer', final_answer)\n",
    "        final_output.append({\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"final_answer\": final_answer\n",
    "        })\n",
    "        print('----------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "febd93ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d9525da",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fcf77b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 33 records to final_output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# … your existing loop that populates final_output …\n",
    "\n",
    "# after you've built final_output:\n",
    "output_path = \"final_output.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(final_output)} records to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a1f1a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 33 records to final_output_base_model.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 1) Load your model & tokenizer\n",
    "model_id = \"h2oai/h2o-danube3-500m-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "device = model.device\n",
    "\n",
    "# 2) Prepare to collect outputs\n",
    "final_output = []\n",
    "max_items = 33  \n",
    "count = 0\n",
    "\n",
    "# 3) Iterate your validation set\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        for segment, query, answer in zip(batch['segment'], batch['query'],  batch['answer']):            \n",
    "            count += 1\n",
    "            if count > max_items:\n",
    "                break\n",
    "            query = segment+query\n",
    "            # tokenize + move to device\n",
    "            input_ids = tokenizer(query, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "            # generate\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                # max_length=100,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            # record\n",
    "            final_output.append({\n",
    "                \"query\": query,\n",
    "                \"answer\": answer,\n",
    "                \"generated_text\": generated_text\n",
    "            })\n",
    "\n",
    "        if count > max_items:\n",
    "            break\n",
    "\n",
    "\n",
    "output_path = \"final_output_base_model.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(final_output)} records to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install bert-score if you haven't already:\n",
    "# pip install bert-score\n",
    "\n",
    "import json\n",
    "from bert_score import score\n",
    "\n",
    "# 1) Load your data\n",
    "with open(\"final_output_base_model.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2) Extract lists of references and candidates\n",
    "#    adjust the keys if yours are named differently\n",
    "refs  = [item[\"answer\"]             for item in data]\n",
    "cands = [item.get(\"final_answer\",  item.get(\"generated_text\")) for item in data]\n",
    "\n",
    "# 3) Compute BERTScore\n",
    "#    lang=\"en\" works for English; you can pick model_type=\"roberta-large\" etc.\n",
    "P, R, F1 = score(\n",
    "    cands,\n",
    "    refs,\n",
    "    lang=\"en\",\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    batch_size=32,\n",
    "    verbose=True,\n",
    "    rescale_with_baseline=True\n",
    ")\n",
    "\n",
    "# 4) Attach scores back to each record\n",
    "for i, item in enumerate(data):\n",
    "    item[\"bert_score_precision\"] = P[i].item()\n",
    "    item[\"bert_score_recall\"]    = R[i].item()\n",
    "    item[\"bert_score_f1\"]        = F1[i].item()\n",
    "\n",
    "# 5) Dump augmented data to a new file\n",
    "out_path = \"final_output_with_bertscore_base_model.json\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Computed BERTScore for {len(data)} examples and wrote to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff394527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c1af7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: -0.1566\n",
      "Average Recall   : -0.0192\n",
      "Average F1       : -0.0986\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import statistics\n",
    "\n",
    "# 1) Load your augmented JSON\n",
    "with open(\"final_output_with_bertscore.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2) Gather all scores\n",
    "precisions = [item[\"bert_score_precision\"] for item in data]\n",
    "recalls    = [item[\"bert_score_recall\"]    for item in data]\n",
    "f1s        = [item[\"bert_score_f1\"]        for item in data]\n",
    "\n",
    "# 3) Compute and print averages\n",
    "print(f\"Average Precision: {statistics.mean(precisions):.4f}\")\n",
    "print(f\"Average Recall   : {statistics.mean(recalls):.4f}\")\n",
    "print(f\"Average F1       : {statistics.mean(f1s):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d611daa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: -0.2754\n",
      "Average Recall   : 0.2099\n",
      "Average F1       : -0.1487\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import statistics\n",
    "\n",
    "# 1) Load your augmented JSON\n",
    "with open(\"/kaggle/working/final_output_with_bertscore_base_model.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2) Gather all scores\n",
    "precisions = [item[\"bert_score_precision\"] for item in data]\n",
    "recalls    = [item[\"bert_score_recall\"]    for item in data]\n",
    "f1s        = [item[\"bert_score_f1\"]        for item in data]\n",
    "\n",
    "# 3) Compute and print averages\n",
    "print(f\"Average Precision: {statistics.mean(precisions):.4f}\")\n",
    "print(f\"Average Recall   : {statistics.mean(recalls):.4f}\")\n",
    "print(f\"Average F1       : {statistics.mean(f1s):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14dc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
